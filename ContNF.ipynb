{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "#!pip install torchdyn\n",
    "#!pip install torchdiffeq\n",
    "#!pip install POT\n",
    "#!pip install tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot as pot\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "from torchdyn.datasets import generate_gaussians, generate_moons, generate_spirals\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                        DATA FOLDER                    '"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"                        DATA FOLDER                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy data sampler\n",
    "\n",
    "def sample_normal(n, mu=torch.zeros(2), sigma=1):\n",
    "    distr = torch.distributions.MultivariateNormal(mu, sigma*torch.eye(2))\n",
    "    return distr.sample((n,))\n",
    "\n",
    "def sample_gaussians(n, n_gaussians=8, radius=4):\n",
    "    num = int(n/n_gaussians)+1\n",
    "    x0, _ = generate_gaussians(n_samples=num, n_gaussians=n_gaussians, radius=radius)\n",
    "    return x0[:n]\n",
    "\n",
    "def sample_moons(n):\n",
    "    x0, _ = generate_moons(n, noise=0.2)\n",
    "    return x0 * 3 - 1\n",
    "\n",
    "def sample_spirals(n):\n",
    "    x0, _ = generate_spirals(int(n/2) , dim=2, inner_radius=.5, outer_radius=1)\n",
    "    return x0\n",
    "\n",
    "class sampler():\n",
    "    def __init__(self, dataset: str):\n",
    "        if dataset == \"normal\":\n",
    "            self.sampler = sample_normal\n",
    "        elif dataset == \"gaussians\":\n",
    "            self.sampler = sample_gaussians\n",
    "        elif dataset == \"moons\":\n",
    "            self.sampler = sample_moons\n",
    "        elif dataset == \"spirals\":\n",
    "            self.sampler = sample_spirals\n",
    "        else:\n",
    "            raise Exception(\"Selected Dataset not supported. Choose between normal, gaussians, moons or spirals\")\n",
    "\n",
    "    \n",
    "    def __call__(self, n: int):\n",
    "        return self.sampler(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST test/train loader\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILS \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#chooses appropriate timesteps for numerical ODE solver\n",
    "def choose_timesteps(method: str):\n",
    "    if method in ['euler', 'rk4', 'midpoint', 'explicit_adams', 'implicit_adams']:\n",
    "        return torch.linspace(0, 1, 5).type(torch.float32)\n",
    "    \n",
    "    elif method in ['dopri8', 'dopri5', 'bosh3', 'adaptive_heun']:\n",
    "        return torch.linspace(0, 1, 2).type(torch.float32)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"numerical method not supported\")\n",
    "    \n",
    "\n",
    "#saves the train logs  \n",
    "def save_train_logs(path, param, batch, loss, times):\n",
    "    os.makedirs(path + \"/logs\", exist_ok=True)\n",
    "    file_path = path + \"/logs/train.txt\"\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "\n",
    "        file.write(\"Parameters:\\n\")\n",
    "        for param, value in param.items():\n",
    "            file.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "        if \"mnist\" in path:\n",
    "            headers = [\"epoch\", \"loss\", \"time\"]\n",
    "        else:\n",
    "            headers = [\"batch\", \"loss\", \"time\"]\n",
    "\n",
    "        data = list(zip(batch, loss, times))\n",
    "        file.write(tabulate(data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "\n",
    "#Computes trace Estimate and Frobenius norm of the Jacobian\n",
    "def divergence_approx(f, y, e=None):\n",
    "\n",
    "    samples = []\n",
    "    sqnorms = []\n",
    "    for  e_ in e:\n",
    "        e_dzdx = torch.autograd.grad(f, y, e_, create_graph=True)[0]\n",
    "        n = e_dzdx.view(y.size(0),-1).pow(2).mean(dim=1, keepdim=True)\n",
    "        sqnorms.append(n)\n",
    "        e_dzdx_e = e_dzdx * e_\n",
    "        samples.append(e_dzdx_e.view(y.shape[0], -1).sum(dim=1, keepdim=True))\n",
    "\n",
    "    S = torch.cat(samples, dim=1)\n",
    "    approx_tr_dzdx = S.mean(dim=1)\n",
    "\n",
    "    N = torch.cat(sqnorms, dim=1).mean(dim=1)\n",
    "\n",
    "\n",
    "    return approx_tr_dzdx, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                      MODELS FOLDER                    '"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"                      MODELS FOLDER                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmented ODE's\n",
    "\n",
    "\"\"\"Augmented ODE for Toy dataset\"\"\"\n",
    "class aug_toy(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module, n_iter: int = 1):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        z = states[0].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)                                        \n",
    "            dz_dt = self.odefunc(t.to(device), z)\n",
    "            epsilon = torch.randn(self.n_iter, *z.size()).to(device)\n",
    "            dlog_det_dt, _ = divergence_approx(dz_dt, z, e=epsilon)                \n",
    "\n",
    "            return (dz_dt, dlog_det_dt.unsqueeze(1))\n",
    "            \n",
    "\n",
    "\"\"\"Augmented ODE for MNIST\"\"\"\n",
    "class aug_mnist(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module, n_iter: int = 1):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        z = states[0].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)                                         \n",
    "            dz_dt = self.odefunc(t.to(device), z)\n",
    "            epsilon = torch.randn(self.n_iter, *z.size()).to(device)\n",
    "            dlog_det_dt, _ = divergence_approx(dz_dt, z, e=epsilon)\n",
    "\n",
    "            \n",
    "            return (dz_dt, dlog_det_dt.unsqueeze(1))\n",
    "        \n",
    "\n",
    "\"\"\"regularized Augmented ODE for Toy dataset\"\"\"\n",
    "class reg_aug_toy(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        z = states[0].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)                                         \n",
    "            dz_dt = self.odefunc(t.to(device), z)\n",
    "            epsilon = torch.randn_like(z).to(device)\n",
    "            dlog_det_dt, dn_dt = divergence_approx(dz_dt, z, e=epsilon.unsqueeze(0))\n",
    "            dE_dt = (torch.linalg.vector_norm(dz_dt, dim=1, keepdims=True)**2)                          #log-det of the Jacobian   \n",
    "\n",
    "            return (dz_dt, dlog_det_dt.unsqueeze(1), dE_dt, dn_dt.unsqueeze(1))\n",
    "\n",
    "\n",
    "\"\"\"regularized Augmented ODE for MNIST\"\"\"\n",
    "class reg_aug_mnist(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        z = states[0].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)                                         \n",
    "            dz_dt = self.odefunc(t.to(device), z)\n",
    "            epsilon = torch.randn_like(z).to(device)\n",
    "            dlog_det_dt, dn_dt = divergence_approx(dz_dt, z, e=epsilon.unsqueeze(0))\n",
    "            dE_dt = (torch.linalg.matrix_norm(dz_dt))**2\n",
    "            \n",
    "            return (dz_dt, dlog_det_dt.unsqueeze(1), dE_dt, dn_dt.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NODE WRAPPER CLASS and evaluation metrics\n",
    "\n",
    "class NODE(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, x, traj=False, t=torch.linspace(1, 0, 2).type(torch.float32)):\n",
    "        flow = odeint(self.odefunc, x, t, \n",
    "                              method=\"dopri5\",        \n",
    "                              atol=1e-3,\n",
    "                              rtol=1e-3,)\n",
    "        \n",
    "        if traj:\n",
    "            return flow\n",
    "        else:\n",
    "            return flow[-1]\n",
    "        \n",
    "    def log_likelihood(self, x, n_iter=10, odeint_method=\"dopri5\"):\n",
    "        t = choose_timesteps(odeint_method).to(device)\n",
    "        l0 = torch.zeros(x.size(0), 1).to(device)\n",
    "        initial_values = (x.to(device), l0)\n",
    "\n",
    "        if x.dim() == 2:\n",
    "            normal = torch.distributions.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "            aug_dynamics = aug_toy(self.odefunc, n_iter)\n",
    "        else:\n",
    "            normal = torch.distributions.MultivariateNormal(torch.zeros(28).to(device), torch.eye(28).to(device))\n",
    "            aug_dynamics = aug_mnist(self.odefunc, n_iter)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_t, log_det_t = odeint(aug_dynamics, initial_values, t,\n",
    "                                    method = odeint_method,\n",
    "                                    atol=1e-3,\n",
    "                                    rtol=1e-3,)\n",
    "            logp_x = normal.log_prob(x_t[-1]) + log_det_t[-1]\n",
    "\n",
    "            return logp_x.mean()\n",
    "\n",
    "def flow_length(flow):\n",
    "    diff = flow[1:] - flow[:-1]\n",
    "\n",
    "    if diff.dim() == 3:\n",
    "        dist = torch.linalg.norm(diff, dim=-1)\n",
    "    else:\n",
    "        dist = torch.linalg.norm(diff, dim=(-2,-1)).squeeze()\n",
    "\n",
    "    return torch.mean(dist.sum(dim=0))\n",
    "\n",
    "def wasserstein2(x, y):\n",
    "    bs = x.size(0)\n",
    "    a, b = pot.unif(bs), pot.unif(bs)\n",
    "\n",
    "    if x.dim() > 2:\n",
    "        x = x.reshape(bs, -1)\n",
    "        y = y.reshape(bs, -1)\n",
    "    \n",
    "    M = torch.cdist(x, y)**2\n",
    "    dist = pot.emd2(a, b, M.detach().cpu().numpy(), numItermax=int(1e7))\n",
    "\n",
    "    return dist**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNET2 \n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# The file is copied and modified from \n",
    "# \n",
    "# C.-W. Huang, J. H. Lim, and A. Courville. \n",
    "# A variational perspective on diffusion-based generative models and score matching. \n",
    "# Advances in Neural Information Processing Systems, 2021\n",
    "# \n",
    "# Code from https://github.com/CW-Huang/sdeflow-light\n",
    "\n",
    "\"\"\"\n",
    "Copied and modified from https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/models/unet.py\n",
    "Copied and modified from https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# noinspection PyProtectedMember\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # noinspection PyMethodMayBeStatic\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x) * x\n",
    "\n",
    "\n",
    "def group_norm(out_ch):\n",
    "    return nn.GroupNorm(num_groups=32, num_channels=out_ch, eps=1e-6, affine=True)\n",
    "\n",
    "\n",
    "def upsample(in_ch, with_conv):\n",
    "    up = nn.Sequential()\n",
    "    up.add_module('up_nn', nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "    if with_conv:\n",
    "        up.add_module('up_conv', conv2d(in_ch, in_ch, kernel_size=(3, 3), stride=1))\n",
    "    return up\n",
    "\n",
    "\n",
    "def downsample(in_ch, with_conv):\n",
    "    if with_conv:\n",
    "        down = conv2d(in_ch, in_ch, kernel_size=(3, 3), stride=2)\n",
    "    else:\n",
    "        down = nn.AvgPool2d(2, 2)\n",
    "    return down\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch=None, conv_shortcut=False, dropout=0., normalize=group_norm, act=Swish()):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch if out_ch is not None else in_ch\n",
    "        self.conv_shortcut = conv_shortcut\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "        self.norm1 = normalize(in_ch) if normalize is not None else nn.Identity()\n",
    "        self.conv1 = conv2d(in_ch, out_ch)\n",
    "        self.norm2 = normalize(out_ch) if normalize is not None else nn.Identity()\n",
    "        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0. else nn.Identity()\n",
    "        self.conv2 = conv2d(out_ch, out_ch, init_scale=0.)\n",
    "        if in_ch != out_ch:\n",
    "            if conv_shortcut:\n",
    "                self.shortcut = conv2d(in_ch, out_ch)\n",
    "            else:\n",
    "                self.shortcut = conv2d(in_ch, out_ch, kernel_size=(1, 1), padding=0)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward conv1\n",
    "        h = x\n",
    "        h = self.act(self.norm1(h))\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        # forward conv2\n",
    "        h = self.act(self.norm2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        # shortcut\n",
    "        x = self.shortcut(x)\n",
    "\n",
    "        # combine and return\n",
    "        assert x.shape == h.shape\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    copied modified from https://github.com/voletiv/self-attention-GAN-pytorch/blob/master/sagan_models.py#L29\n",
    "    copied modified from https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/models/unet.py#L66\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, normalize=group_norm):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.attn_q = conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.attn_k = conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.attn_v = conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.proj_out = conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0, init_scale=0.)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        if normalize is not None:\n",
    "            self.norm = normalize(in_channels)\n",
    "        else:\n",
    "            self.norm = nn.Identity()\n",
    "\n",
    "    # noinspection PyUnusedLocal\n",
    "    def forward(self, x):\n",
    "        \"\"\" t is not used \"\"\"\n",
    "        _, C, H, W = x.size()\n",
    "\n",
    "        h = self.norm(x)\n",
    "        q = self.attn_q(h).view(-1, C, H * W)\n",
    "        k = self.attn_k(h).view(-1, C, H * W)\n",
    "        v = self.attn_v(h).view(-1, C, H * W)\n",
    "\n",
    "        attn = torch.bmm(q.permute(0, 2, 1), k) * (int(C) ** (-0.5))\n",
    "        attn = self.softmax(attn)\n",
    "\n",
    "        h = torch.bmm(v, attn.permute(0, 2, 1))\n",
    "        h = h.view(-1, C, H, W)\n",
    "        h = self.proj_out(h)\n",
    "\n",
    "        assert h.shape == x.shape\n",
    "        return x + h\n",
    "\n",
    "\n",
    "def _calculate_correct_fan(tensor, mode):\n",
    "    \"\"\"\n",
    "    copied and modified from https://github.com/pytorch/pytorch/blob/master/torch/nn/init.py#L337\n",
    "    \"\"\"\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out', 'fan_avg']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(\"Mode {} not supported, please use one of {}\".format(mode, valid_modes))\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "\n",
    "def kaiming_uniform_(tensor, gain=1., mode='fan_in'):\n",
    "    r\"\"\"Fills the input `Tensor` with values according to the method\n",
    "    described in `Delving deep into rectifiers: Surpassing human-level\n",
    "    performance on ImageNet classification` - He, K. et al. (2015), using a\n",
    "    uniform distribution. The resulting tensor will have values sampled from\n",
    "    :math:`\\mathcal{U}(-\\text{bound}, \\text{bound})` where\n",
    "\n",
    "    .. math::\n",
    "        \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}\n",
    "\n",
    "    Also known as He initialization.\n",
    "\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        gain: multiplier to the dispersion\n",
    "        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n",
    "            preserves the magnitude of the variance of the weights in the\n",
    "            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n",
    "            backwards pass.\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.kaiming_uniform_(w, mode='fan_in')\n",
    "    \"\"\"\n",
    "    fan = _calculate_correct_fan(tensor, mode)\n",
    "    # gain = calculate_gain(nonlinearity, a)\n",
    "    var = gain / max(1., fan)\n",
    "    bound = math.sqrt(3.0 * var)  # Calculate uniform bounds from standard deviation\n",
    "    with torch.no_grad():\n",
    "        return tensor.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def variance_scaling_init_(tensor, scale):\n",
    "    return kaiming_uniform_(tensor, gain=1e-10 if scale == 0 else scale, mode='fan_avg')\n",
    "\n",
    "\n",
    "def dense(in_channels, out_channels, init_scale=1.):\n",
    "    lin = nn.Linear(in_channels, out_channels)\n",
    "    variance_scaling_init_(lin.weight, scale=init_scale)\n",
    "    nn.init.zeros_(lin.bias)\n",
    "    return lin\n",
    "\n",
    "\n",
    "def conv2d(in_planes, out_planes, kernel_size=(3, 3), stride=1, dilation=1, padding=1, bias=True, padding_mode='zeros',\n",
    "           init_scale=1.):\n",
    "    conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation,\n",
    "                     bias=bias, padding_mode=padding_mode)\n",
    "    variance_scaling_init_(conv.weight, scale=init_scale)\n",
    "    if bias:\n",
    "        nn.init.zeros_(conv.bias)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def get_sinusoidal_positional_embedding(timesteps: torch.LongTensor, embedding_dim: int):\n",
    "    \"\"\"\n",
    "    Copied and modified from\n",
    "        https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/nn.py#L90\n",
    "\n",
    "    From Fairseq in\n",
    "        https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sinusoidal_positional_embedding.py#L15\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.size()) == 1\n",
    "    timesteps = timesteps.to(torch.get_default_dtype())\n",
    "    device = timesteps.device\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float, device=device) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)  # bsz x embd\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = F.pad(emb, (0, 1), \"constant\", 0)\n",
    "    assert list(emb.size()) == [timesteps.size(0), embedding_dim]\n",
    "    return emb\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels=1,\n",
    "                 input_height=28,\n",
    "                 ch=32,\n",
    "                 output_channels=None,\n",
    "                 ch_mult=(1, 2, 4),\n",
    "                 num_res_blocks=2,\n",
    "                 attn_resolutions=(16,),\n",
    "                 dropout=0.,\n",
    "                 resamp_with_conv=True,\n",
    "                 act=Swish(),\n",
    "                 normalize=group_norm,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.input_height = input_height\n",
    "        self.ch = ch\n",
    "        self.output_channels = output_channels = input_channels if output_channels is None else output_channels\n",
    "        self.ch_mult = ch_mult\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attn_resolutions = attn_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.resamp_with_conv = resamp_with_conv\n",
    "        self.act = act\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # init\n",
    "        self.num_resolutions = num_resolutions = len(ch_mult)\n",
    "        in_ht = input_height\n",
    "        in_ch = input_channels\n",
    "        assert in_ht % 2 ** (num_resolutions - 1) == 0, \"input_height doesn't satisfy the condition\"\n",
    "\n",
    "\n",
    "        # Downsampling\n",
    "        self.begin_conv = conv2d(in_ch, ch)\n",
    "        unet_chs = [ch]\n",
    "        in_ht = in_ht\n",
    "        in_ch = ch\n",
    "        down_modules = []\n",
    "        for i_level in range(num_resolutions):\n",
    "            # Residual blocks for this resolution\n",
    "            block_modules = {}\n",
    "            out_ch = ch * ch_mult[i_level]\n",
    "            for i_block in range(num_res_blocks):\n",
    "                block_modules['{}a_{}a_block'.format(i_level, i_block)] = \\\n",
    "                    ResidualBlock(\n",
    "                        in_ch=in_ch,\n",
    "                        out_ch=out_ch,\n",
    "                        dropout=dropout,\n",
    "                        act=act,\n",
    "                        normalize=normalize,\n",
    "                    )\n",
    "                if in_ht in attn_resolutions:\n",
    "                    block_modules['{}a_{}b_attn'.format(i_level, i_block)] = SelfAttention(out_ch, normalize=normalize)\n",
    "                unet_chs += [out_ch]\n",
    "                in_ch = out_ch\n",
    "            # Downsample\n",
    "            if i_level != num_resolutions - 1:\n",
    "                block_modules['{}b_downsample'.format(i_level)] = downsample(out_ch, with_conv=resamp_with_conv)\n",
    "                in_ht //= 2\n",
    "                unet_chs += [out_ch]\n",
    "            # convert list of modules to a module list, and append to a list\n",
    "            down_modules += [nn.ModuleDict(block_modules)]\n",
    "        # conver to a module list\n",
    "        self.down_modules = nn.ModuleList(down_modules)\n",
    "\n",
    "        # Middle\n",
    "        mid_modules = []\n",
    "        mid_modules += [\n",
    "            ResidualBlock(in_ch, out_ch=in_ch, dropout=dropout, act=act, normalize=normalize)]\n",
    "        mid_modules += [SelfAttention(in_ch, normalize=normalize)]\n",
    "        mid_modules += [\n",
    "            ResidualBlock(in_ch, out_ch=in_ch, dropout=dropout, act=act, normalize=normalize)]\n",
    "        self.mid_modules = nn.ModuleList(mid_modules)\n",
    "\n",
    "        # Upsampling\n",
    "        up_modules = []\n",
    "        for i_level in reversed(range(num_resolutions)):\n",
    "            # Residual blocks for this resolution\n",
    "            block_modules = {}\n",
    "            out_ch = ch * ch_mult[i_level]\n",
    "            for i_block in range(num_res_blocks + 1):\n",
    "                block_modules['{}a_{}a_block'.format(i_level, i_block)] = \\\n",
    "                    ResidualBlock(\n",
    "                        in_ch=in_ch + unet_chs.pop(),\n",
    "                        out_ch=out_ch,\n",
    "                        dropout=dropout,\n",
    "                        act=act,\n",
    "                        normalize=normalize)\n",
    "                if in_ht in attn_resolutions:\n",
    "                    block_modules['{}a_{}b_attn'.format(i_level, i_block)] = SelfAttention(out_ch, normalize=normalize)\n",
    "                in_ch = out_ch\n",
    "            # Upsample\n",
    "            if i_level != 0:\n",
    "                block_modules['{}b_upsample'.format(i_level)] = upsample(out_ch, with_conv=resamp_with_conv)\n",
    "                in_ht *= 2\n",
    "            # convert list of modules to a module list, and append to a list\n",
    "            up_modules += [nn.ModuleDict(block_modules)]\n",
    "        # conver to a module list\n",
    "        self.up_modules = nn.ModuleList(up_modules)\n",
    "        assert not unet_chs\n",
    "\n",
    "        # End\n",
    "        self.end_conv = nn.Sequential(\n",
    "            normalize(in_ch),\n",
    "            self.act,\n",
    "            conv2d(in_ch, output_channels, init_scale=0.),\n",
    "        )\n",
    "\n",
    "    # noinspection PyMethodMayBeStatic\n",
    "    def _compute_cond_module(self, module, x):\n",
    "        for m in module:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "    # noinspection PyArgumentList,PyShadowingNames\n",
    "    def forward(self, t, x):\n",
    "        del t\n",
    "        # Init\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        # Downsampling\n",
    "        hs = [self.begin_conv(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            # Residual blocks for this resolution\n",
    "            block_modules = self.down_modules[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                resnet_block = block_modules['{}a_{}a_block'.format(i_level, i_block)]\n",
    "                h = resnet_block(hs[-1])\n",
    "                if h.size(2) in self.attn_resolutions:\n",
    "                    attn_block = block_modules['{}a_{}b_attn'.format(i_level, i_block)]\n",
    "                    h = attn_block(h)\n",
    "                hs.append(h)\n",
    "            # Downsample\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                downsample = block_modules['{}b_downsample'.format(i_level)]\n",
    "                hs.append(downsample(hs[-1]))\n",
    "\n",
    "        # Middle\n",
    "        h = hs[-1]\n",
    "        h = self._compute_cond_module(self.mid_modules, h)\n",
    "\n",
    "        # Upsampling\n",
    "        for i_idx, i_level in enumerate(reversed(range(self.num_resolutions))):\n",
    "            # Residual blocks for this resolution\n",
    "            block_modules = self.up_modules[i_idx]\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                resnet_block = block_modules['{}a_{}a_block'.format(i_level, i_block)]\n",
    "                h = resnet_block(torch.cat([h, hs.pop()], axis=1))\n",
    "                if h.size(2) in self.attn_resolutions:\n",
    "                    attn_block = block_modules['{}a_{}b_attn'.format(i_level, i_block)]\n",
    "                    h = attn_block(h)\n",
    "            # Upsample\n",
    "            if i_level != 0:\n",
    "                upsample = block_modules['{}b_upsample'.format(i_level)]\n",
    "                h = upsample(h)\n",
    "        assert not hs\n",
    "\n",
    "        # End\n",
    "        h = self.end_conv(h)\n",
    "        assert list(h.size()) == [x.size(0), self.output_channels, x.size(2), x.size(3)]\n",
    "        return h\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model = UNet()\n",
    "    \n",
    "    model(1, torch.randn((1,1,28,28)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=None, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        if out_dim is None:\n",
    "            out_dim = in_dim\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim + 1, hidden_dim),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if t.dim()==0:\n",
    "            t = t.expand(x.size(0),)\n",
    "        x = torch.cat((x, t[:,None]),dim=-1) \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                    Training Folder                    '"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"                    Training Folder                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILS    \n",
    "    \n",
    "def save_train_logs(path, param, batch, loss, times):\n",
    "    os.makedirs(path + \"/logs\", exist_ok=True)\n",
    "    file_path = path + \"/logs/train.txt\"\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "\n",
    "        file.write(\"Parameters:\\n\")\n",
    "        for param, value in param.items():\n",
    "            file.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "        if \"mnist\" in path:\n",
    "            headers = [\"epoch\", \"loss\", \"time\"]\n",
    "        else:\n",
    "            headers = [\"batch\", \"loss\", \"time\"]\n",
    "\n",
    "        data = list(zip(batch, loss, times))\n",
    "        file.write(tabulate(data, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal Transport Sampler\n",
    "\n",
    "\"\"\"From https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/optimal_transport.py\"\"\"\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class OTPlanSampler:\n",
    "    \"\"\"OTPlanSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean\n",
    "    cost) with different implementations of the plan calculation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: str,\n",
    "        reg: float = 0.05,\n",
    "        reg_m: float = 1.0,\n",
    "        normalize_cost: bool = False,\n",
    "        warn: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the OTPlanSampler class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method: str\n",
    "            choose which optimal transport solver you would like to use.\n",
    "            Currently supported are [\"exact\", \"sinkhorn\", \"unbalanced\",\n",
    "            \"partial\"] OT solvers.\n",
    "        reg: float, optional\n",
    "            regularization parameter to use for Sinkhorn-based iterative solvers.\n",
    "        reg_m: float, optional\n",
    "            regularization weight for unbalanced Sinkhorn-knopp solver.\n",
    "        normalize_cost: bool, optional\n",
    "            normalizes the cost matrix so that the maximum cost is 1. Helps\n",
    "            stabilize Sinkhorn-based solvers. Should not be used in the vast\n",
    "            majority of cases.\n",
    "        warn: bool, optional\n",
    "            if True, raises a warning if the algorithm does not converge\n",
    "        \"\"\"\n",
    "        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n",
    "        # M is a cost matrix\n",
    "        if method == \"exact\":\n",
    "            self.ot_fn = pot.emd\n",
    "        elif method == \"sinkhorn\":\n",
    "            self.ot_fn = partial(pot.sinkhorn, reg=reg)\n",
    "        elif method == \"unbalanced\":\n",
    "            self.ot_fn = partial(pot.unbalanced.sinkhorn_knopp_unbalanced, reg=reg, reg_m=reg_m)\n",
    "        elif method == \"partial\":\n",
    "            self.ot_fn = partial(pot.partial.entropic_partial_wasserstein, reg=reg)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        self.reg = reg\n",
    "        self.reg_m = reg_m\n",
    "        self.normalize_cost = normalize_cost\n",
    "        self.warn = warn\n",
    "\n",
    "    def get_map(self, x0, x1):\n",
    "        \"\"\"Compute the OT plan (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : numpy array, shape (bs, bs)\n",
    "            represents the OT plan between minibatches\n",
    "        \"\"\"\n",
    "        a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n",
    "        if x0.dim() > 2:\n",
    "            x0 = x0.reshape(x0.shape[0], -1)\n",
    "        if x1.dim() > 2:\n",
    "            x1 = x1.reshape(x1.shape[0], -1)\n",
    "        x1 = x1.reshape(x1.shape[0], -1)\n",
    "        M = torch.cdist(x0, x1) ** 2\n",
    "        if self.normalize_cost:\n",
    "            M = M / M.max()  # should not be normalized when using minibatches\n",
    "        p = self.ot_fn(a, b, M.detach().cpu().numpy())\n",
    "        if not np.all(np.isfinite(p)):\n",
    "            print(\"ERROR: p is not finite\")\n",
    "            print(p)\n",
    "            print(\"Cost mean, max\", M.mean(), M.max())\n",
    "            print(x0, x1)\n",
    "        if np.abs(p.sum()) < 1e-8:\n",
    "            if self.warn:\n",
    "                warnings.warn(\"Numerical errors in OT plan, reverting to uniform plan.\")\n",
    "            p = np.ones_like(p) / p.size\n",
    "        return p\n",
    "\n",
    "    def sample_map(self, pi, batch_size, replace=True):\n",
    "        r\"\"\"Draw source and target samples from pi  $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : numpy array, shape (bs, bs)\n",
    "            represents the source minibatch\n",
    "        batch_size : int\n",
    "            represents the OT plan between minibatches\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (i_s, i_j) : tuple of numpy arrays, shape (bs, bs)\n",
    "            represents the indices of source and target data samples from $\\pi$\n",
    "        \"\"\"\n",
    "        p = pi.flatten()\n",
    "        p = p / p.sum()\n",
    "        choices = np.random.choice(\n",
    "            pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=replace\n",
    "        )\n",
    "        return np.divmod(choices, pi.shape[1])\n",
    "\n",
    "    def sample_plan(self, x0, x1, replace=True):\n",
    "        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        x1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        \"\"\"\n",
    "        pi = self.get_map(x0, x1)\n",
    "        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n",
    "        return x0[i], x1[j]\n",
    "\n",
    "    def sample_plan_with_labels(self, x0, x1, y0=None, y1=None, replace=True):\n",
    "        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch and draw source and target labeled samples from pi $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        y0 : Tensor, shape (bs)\n",
    "            represents the source label minibatch\n",
    "        y1 : Tensor, shape (bs)\n",
    "            represents the target label minibatch\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        x1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch drawn from $\\pi$\n",
    "        y0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source label minibatch drawn from $\\pi$\n",
    "        y1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the target label minibatch drawn from $\\pi$\n",
    "        \"\"\"\n",
    "        pi = self.get_map(x0, x1)\n",
    "        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n",
    "        return (\n",
    "            x0[i],\n",
    "            x1[j],\n",
    "            y0[i] if y0 is not None else None,\n",
    "            y1[j] if y1 is not None else None,\n",
    "        )\n",
    "\n",
    "    def sample_trajectory(self, X):\n",
    "        \"\"\"Compute the OT flow between different sample populations moving from the source\n",
    "        to the target distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Tensor, (bs, times, *dim)\n",
    "            different populations of samples moving from the source to the target distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        to_return : Tensor, (bs, times, *dim)\n",
    "            represents the OT sampled trajectories over time.\n",
    "        \"\"\"\n",
    "        times = X.shape[1]\n",
    "        pis = []\n",
    "        for t in range(times - 1):\n",
    "            pis.append(self.get_map(X[:, t], X[:, t + 1]))\n",
    "\n",
    "        indices = [np.arange(X.shape[0])]\n",
    "        for pi in pis:\n",
    "            j = []\n",
    "            for i in indices[-1]:\n",
    "                j.append(np.random.choice(pi.shape[1], p=pi[i] / pi[i].sum()))\n",
    "            indices.append(np.array(j))\n",
    "\n",
    "        to_return = []\n",
    "        for t in range(times):\n",
    "            to_return.append(X[:, t][indices[t]])\n",
    "        to_return = np.stack(to_return, axis=1)\n",
    "        return to_return\n",
    "\n",
    "\"\"\"Implements Conditional Flow Matcher Losses. From https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/conditional_flow_matching.py\"\"\"\n",
    "\n",
    "# Author: Alex Tong\n",
    "#         Kilian Fatras\n",
    "#         +++\n",
    "# License: MIT License\n",
    "\n",
    "import warnings\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def pad_t_like_x(t, x):\n",
    "    \"\"\"Function to reshape the time vector t by the number of dimensions of x.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor, shape (bs, *dim)\n",
    "        represents the source minibatch\n",
    "    t : FloatTensor, shape (bs)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t : Tensor, shape (bs, number of x dimensions)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    x: Tensor (bs, C, W, H)\n",
    "    t: Vector (bs)\n",
    "    pad_t_like_x(t, x): Tensor (bs, 1, 1, 1)\n",
    "    \"\"\"\n",
    "    if isinstance(t, (float, int)):\n",
    "        return t\n",
    "    return t.reshape(-1, *([1] * (x.dim() - 1)))\n",
    "\n",
    "\n",
    "class ConditionalFlowMatcher:\n",
    "    \"\"\"Base class for conditional flow matching methods. This class implements the independent\n",
    "    conditional flow matching methods from [1] and serves as a parent class for all other flow\n",
    "    matching methods.\n",
    "\n",
    "    It implements:\n",
    "    - Drawing data from gaussian probability path N(t * x1 + (1 - t) * x0, sigma) function\n",
    "    - conditional flow matching ut(x1|x0) = x1 - x0\n",
    "    - score function $\\nabla log p_t(x|x0, x1)$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Union[float, int] = 0.0):\n",
    "        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma : Union[float, int]\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def compute_mu_t(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Compute the mean of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mean mu_t: t * x1 + (1 - t) * x0\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        t = pad_t_like_x(t, x0)\n",
    "        return t * x1 + (1 - t) * x0\n",
    "\n",
    "    def compute_sigma_t(self, t):\n",
    "        \"\"\"\n",
    "        Compute the standard deviation of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        standard deviation sigma\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        del t\n",
    "        return self.sigma\n",
    "\n",
    "    def sample_xt(self, x0, x1, t, epsilon):\n",
    "        \"\"\"\n",
    "        Draw a sample from the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "        epsilon : Tensor, shape (bs, *dim)\n",
    "            noise sample from N(0, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        mu_t = self.compute_mu_t(x0, x1, t)\n",
    "        sigma_t = self.compute_sigma_t(t)\n",
    "        sigma_t = pad_t_like_x(sigma_t, x0)\n",
    "        return mu_t + sigma_t * epsilon\n",
    "\n",
    "    def compute_conditional_flow(self, x0, x1, t, xt):\n",
    "        \"\"\"\n",
    "        Compute the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        del t, xt\n",
    "        return x1 - x0\n",
    "\n",
    "    def sample_noise_like(self, x):\n",
    "        return torch.randn_like(x)\n",
    "\n",
    "    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n",
    "        \"\"\"\n",
    "        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n",
    "        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        (optionally) t : Tensor, shape (bs)\n",
    "            represents the time levels\n",
    "            if None, drawn from uniform [0,1]\n",
    "        return_noise : bool\n",
    "            return the noise sample epsilon\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "        (optionally) eps: Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        if t is None:\n",
    "            t = torch.rand(x0.shape[0]).type_as(x0)\n",
    "        assert len(t) == x0.shape[0], \"t has to have batch size dimension\"\n",
    "\n",
    "        eps = self.sample_noise_like(x0)\n",
    "        xt = self.sample_xt(x0, x1, t, eps)\n",
    "        ut = self.compute_conditional_flow(x0, x1, t, xt)\n",
    "        if return_noise:\n",
    "            return t, xt, ut, eps\n",
    "        else:\n",
    "            return t, xt, ut\n",
    "\n",
    "    def compute_lambda(self, t):\n",
    "        \"\"\"Compute the lambda function, see Eq.(23) [3].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lambda : score weighting function\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [4] Simulation-free Schrodinger bridges via score and flow matching, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        sigma_t = self.compute_sigma_t(t)\n",
    "        return 2 * sigma_t / (self.sigma**2 + 1e-8)\n",
    "\n",
    "\n",
    "class ExactOptimalTransportConditionalFlowMatcher(ConditionalFlowMatcher):\n",
    "    \"\"\"Child class for optimal transport conditional flow matching method. This class implements\n",
    "    the OT-CFM methods from [1] and inherits the ConditionalFlowMatcher parent class.\n",
    "\n",
    "    It overrides the sample_location_and_conditional_flow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Union[float, int] = 0.0):\n",
    "        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma : Union[float, int]\n",
    "        ot_sampler: exact OT method to draw couplings (x0, x1) (see Eq.(17) [1]).\n",
    "        \"\"\"\n",
    "        super().__init__(sigma)\n",
    "        self.ot_sampler = OTPlanSampler(method=\"exact\")\n",
    "\n",
    "    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n",
    "        r\"\"\"\n",
    "        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n",
    "        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n",
    "        with respect to the minibatch OT plan $\\Pi$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        (optionally) t : Tensor, shape (bs)\n",
    "            represents the time levels\n",
    "            if None, drawn from uniform [0,1]\n",
    "        return_noise : bool\n",
    "            return the noise sample epsilon\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        x0, x1 = self.ot_sampler.sample_plan(x0, x1)\n",
    "        return super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_toy_node(n_batches, batch_size, target='moons', learning_rate=1e-3, seed=22, odeint_method='dopri5'):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    model = MLP().to(device)\n",
    "\n",
    "    target_distr = sampler(target)\n",
    "    initial_distr = torch.distributions.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    batch_save = int(n_batches/100)\n",
    "    batch_loss = 0\n",
    "    start = time.time()\n",
    "    path = target + \"/node\"\n",
    "    os.makedirs(path + \"/models\", exist_ok=True)\n",
    "\n",
    "    batches, losses, train_time = [], [], []\n",
    "\n",
    "    for i in tqdm(range(1, n_batches+1), desc=\"Training Batches\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = target_distr(batch_size).to(device)\n",
    "        x.requires_grad = True\n",
    "\n",
    "        t = choose_timesteps(odeint_method).to(device)\n",
    "        l0 = torch.zeros((x.size(0),1), requires_grad=True).to(device)\n",
    "        initial_values = (x, l0)\n",
    "\n",
    "        augmented_dynamics = aug_toy(model)\n",
    "\n",
    "        z_t, log_det = odeint(augmented_dynamics, initial_values, t,\n",
    "                                        method=odeint_method,        \n",
    "                                        atol=1e-3,\n",
    "                                        rtol=1e-3,)\n",
    "\n",
    "\n",
    "        z1, l1 = z_t[-1], log_det[-1]\n",
    "\n",
    "        logp_x = initial_distr.log_prob(z1) + l1\n",
    "        loss = -logp_x.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss\n",
    "\n",
    "        if i%batch_save == 0:\n",
    "            batch_loss = batch_loss/batch_save\n",
    "            elapsed_time = time.time() - start\n",
    "            \n",
    "            batches.append(i)\n",
    "            losses.append(batch_loss)\n",
    "            train_time.append(elapsed_time)\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(path + \"/models\", f\"{i}_model.pt\"))\n",
    "            print(f\"\\nbatch {i}, Loss: {batch_loss}\\n\")\n",
    "        \n",
    "            if batch_loss > 1e+25:\n",
    "                print(f\"\\n training collapsed at batch {i}\")\n",
    "                break\n",
    "\n",
    "            batch_loss = 0\n",
    "\n",
    "    print(\"finished training\\n\")\n",
    "\n",
    "    return path, batches, losses, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_toy_rnode(n_batches, batch_size, target='moons', learning_rate=1e-3, seed=22, lambda_k=.1, lambda_j=.1, odeint_method='dopri5'):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    model = MLP().to(device)\n",
    "\n",
    "    target_distr = sampler(target)\n",
    "    initial_distr = torch.distributions.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    batch_save = int(n_batches/100)\n",
    "    batch_loss = 0\n",
    "    start = time.time()\n",
    "    path = target + \"/rnode\"\n",
    "    os.makedirs(path + \"/models\", exist_ok=True)\n",
    "\n",
    "    batches, losses, train_time = [], [], []\n",
    "\n",
    "    for i in tqdm(range(1, n_batches+1), desc=\"Training Batches\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = target_distr(batch_size).to(device)\n",
    "        x.requires_grad = True\n",
    "\n",
    "        t = choose_timesteps(odeint_method).to(device)\n",
    "        l0 = torch.zeros((x.size(0),1), requires_grad=True).to(device)\n",
    "        kin_E0 = torch.zeros((x.size(0),1), requires_grad=True).to(device)\n",
    "        n0 = torch.zeros((x.size(0),1), requires_grad=True).to(device)\n",
    "        initial_values = (x, l0, kin_E0, n0)\n",
    "\n",
    "        augmented_dynamics = reg_aug_toy(model)\n",
    "\n",
    "        z_t, log_det, E_t, n_t = odeint(augmented_dynamics, initial_values, t,\n",
    "                                        method=odeint_method,        \n",
    "                                        atol=1e-3,\n",
    "                                        rtol=1e-3,)\n",
    "\n",
    "\n",
    "        z1, l1, kin_E1, n1 = z_t[-1], log_det[-1], E_t[-1], n_t[-1]\n",
    "\n",
    "        logp_x = initial_distr.log_prob(z1) + l1 - lambda_k * kin_E1 - lambda_j * n1\n",
    "        loss = -logp_x.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss\n",
    "\n",
    "        if i%batch_save == 0:\n",
    "            batch_loss = batch_loss/batch_save\n",
    "            elapsed_time = time.time() - start\n",
    "            \n",
    "            batches.append(i)\n",
    "            losses.append(batch_loss)\n",
    "            train_time.append(elapsed_time)\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(path + \"/models\", f\"{i}_model.pt\"))\n",
    "            print(f\"\\nbatch {i}, Loss: {batch_loss}\\n\")\n",
    "            batch_loss = 0\n",
    "\n",
    "    print(\"finished training\\n\")\n",
    "\n",
    "    return path, batches, losses, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_toy_cfm(n_batches, batch_size, target='moons', learning_rate=1e-3, seed=22, sigma=0):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    model = MLP().to(device)\n",
    "\n",
    "    target_distr = sampler(target)\n",
    "    initial_distr = torch.distributions.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    FM = ExactOptimalTransportConditionalFlowMatcher(sigma=sigma)\n",
    "    model.train()\n",
    "\n",
    "    batch_save = int(n_batches/100)\n",
    "    batch_loss = 0\n",
    "    start = time.time()\n",
    "    path = target + \"/cfm\"\n",
    "    os.makedirs(path + \"/models\", exist_ok=True)\n",
    "\n",
    "    batches, losses, train_time = [], [], []\n",
    "\n",
    "    for i in tqdm(range(1, n_batches+1), desc=\"Training Batches\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x0, x1 = initial_distr.sample((batch_size,)).to(device), target_distr(batch_size).to(device)\n",
    "        t, xt, ut = FM.sample_location_and_conditional_flow(x1, x0)\n",
    "        \n",
    "        loss = torch.mean((model(t,xt) - ut)**2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss\n",
    "\n",
    "        if i%batch_save == 0:\n",
    "            batch_loss = batch_loss/batch_save\n",
    "            elapsed_time = time.time() - start\n",
    "            \n",
    "            batches.append(i)\n",
    "            losses.append(batch_loss)\n",
    "            train_time.append(elapsed_time)\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(path + \"/models\", f\"{i}_model.pt\"))\n",
    "            print(f\"\\nbatch {i}, Loss: {batch_loss}\\n\")\n",
    "            batch_loss = 0\n",
    "\n",
    "    print(\"finished training\\n\")\n",
    "\n",
    "    return path, batches, losses, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_node(n_epochs, learning_rate=1e-3, seed=22, odeint_method='dopri5'):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model = UNet().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    initial_distr = torch.distributions.MultivariateNormal(torch.zeros(28).to(device), torch.eye(28).to(device))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    path = \"mnist/node\"\n",
    "    os.makedirs(path + \"/models\", exist_ok=True)\n",
    "    epochs, losses, train_time = [], [], []\n",
    "    start = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{n_epochs}')\n",
    "        num = 0\n",
    "        for i, (samples, labels) in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            #samples.requires_grad = True\n",
    "\n",
    "            x0 = samples.to(device)\n",
    "            \n",
    "            t = choose_timesteps(odeint_method).to(device)\n",
    "            l0 = torch.zeros((x0.size(0),1), requires_grad=True).to(device)\n",
    "            initial_values = (x0, l0)\n",
    "\n",
    "            augmented_dynamics = aug_mnist(model)\n",
    "\n",
    "            z_t, log_det = odeint(augmented_dynamics, initial_values, t, \n",
    "                                            method=odeint_method, \n",
    "                                            rtol=1e-5, \n",
    "                                            atol=1e-5,)\n",
    "            \n",
    "            z1, l1 = z_t[-1], log_det[-1]\n",
    "\n",
    "            logp_x = initial_distr.log_prob(z1) + l1\n",
    "            loss = -logp_x.mean()\n",
    "            print(f'\\n batch {i}, loss: {loss}')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            num += 1\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(path + \"/models\", f\"{epoch}_model.pt\"))\n",
    "        print(f\"\\nloss: {epoch_loss/num}\\n\")\n",
    "        \n",
    "        elapsed_time = time.time() - start\n",
    "        \n",
    "        epochs.append(epoch)\n",
    "        losses.append(epoch_loss/num)\n",
    "        train_time.append(elapsed_time)\n",
    "        \n",
    "    print(\"finished training\\n\")\n",
    "\n",
    "    return path, epochs, losses, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_rnode(n_epochs, learning_rate=1e-3, seed=22, lambda_k=.1, lambda_j=.1, odeint_method='dopri5'):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model = UNet().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    initial_distr = torch.distributions.MultivariateNormal(torch.zeros(28).to(device), torch.eye(28).to(device))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    path = \"mnist/rnode\"\n",
    "    os.makedirs(path + \"/models\", exist_ok=True)\n",
    "    epochs, losses, train_time = [], [], []\n",
    "    start = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{n_epochs}')\n",
    "        num = 0\n",
    "        for i, (samples, labels) in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            #samples.requires_grad = True\n",
    "\n",
    "            x0 = samples.to(device)\n",
    "            \n",
    "            t = choose_timesteps(odeint_method).to(device)\n",
    "            l0 = torch.zeros((x0.size(0),1), requires_grad=True).to(device)\n",
    "            kin_E0 = torch.zeros((x0.size(0),1), requires_grad=True).to(device)\n",
    "            n0 = torch.zeros((x0.size(0),1), requires_grad=True).to(device)\n",
    "            initial_values = (x0, l0, kin_E0, n0)\n",
    "\n",
    "            augmented_dynamics = reg_aug_mnist(model)\n",
    "\n",
    "            z_t, log_det, E_t, n_t = odeint(augmented_dynamics, initial_values, t, \n",
    "                                            method=odeint_method, \n",
    "                                            rtol=1e-5, \n",
    "                                            atol=1e-5,)\n",
    "\n",
    "            z1, l1, kin_E1, n1 = z_t[-1], log_det[-1], E_t[-1], n_t[-1]\n",
    "\n",
    "            logp_x = initial_distr.log_prob(z1) + l1 - lambda_k * kin_E1 - lambda_j * n1\n",
    "            loss = -logp_x.mean()\n",
    "            print(f'\\n batch {i}, loss: {loss}')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            num += 1\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(path + \"/models\", f\"{epoch}_model.pt\"))\n",
    "        print(f\"\\nloss: {epoch_loss/num}\\n\")\n",
    "        \n",
    "        elapsed_time = time.time() - start\n",
    "        \n",
    "        epochs.append(epoch)\n",
    "        losses.append(epoch_loss/num)\n",
    "        train_time.append(elapsed_time)\n",
    "        \n",
    "    print(\"finished training\\n\")\n",
    "\n",
    "    return path, epochs, losses, train_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_cfm(n_epochs, learning_rate=1e-3, seed=22, sigma=0):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model = UNet().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    FM = ExactOptimalTransportConditionalFlowMatcher(sigma=sigma)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    path = \"mnist/cfm\"\n",
    "    os.makedirs(path + \"/models\", exist_ok=True)\n",
    "    epochs, losses, train_time = [], [], []\n",
    "    start = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{n_epochs}')\n",
    "        num = 0\n",
    "        for i, (samples, labels) in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x0 = torch.randn_like(samples).to(device)\n",
    "            t, xt, ut = FM.sample_location_and_conditional_flow(samples.to(device), x0)\n",
    "            vt = model(t, xt)\n",
    "            loss = torch.mean((vt - ut) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            num += 1\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(path + \"/models\", f\"{epoch}_model.pt\"))\n",
    "        print(f\"\\nloss: {epoch_loss/num}\\n\")\n",
    "        \n",
    "        elapsed_time = time.time() - start\n",
    "        \n",
    "        epochs.append(epoch)\n",
    "        losses.append(epoch_loss/num)\n",
    "        train_time.append(elapsed_time)\n",
    "        \n",
    "    print(\"finished training\\n\")\n",
    "\n",
    "    return path, epochs, losses, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, training, \n",
    "                n_epochs = 1,       \n",
    "                n_batches = 10,   \n",
    "                batch_size = 10,\n",
    "                odeint_method = 'rk4',\n",
    "                lambda_k = .1,       \n",
    "                lambda_j = .1,       \n",
    "                sigma = 0.1,):         \n",
    "    \"\"\" \n",
    "    Parameters for Training\n",
    "        dataset: ['moons', 'gaussians', 'circles', 'mnist']\n",
    "        training: ['node', 'rnode', 'cfm']\n",
    "\n",
    "    MNIST datset parameters\n",
    "        n_epochs: number of epochs for training    \n",
    "\n",
    "    TOY dataset parameters\n",
    "        n_batches: number of batches for training\n",
    "        batch_size: size of training batches\n",
    "\n",
    "    RNODE parameters\n",
    "        odeint_method: ['euler', 'rk4', 'dopri5', 'dopri8']\n",
    "        lambda_k: regularization constant for the kinetic energy\n",
    "        lambda_j: regularization constant for the frobenius norm\n",
    "\n",
    "    CFM parameters\n",
    "        sigma: variance of gaussian probability paths\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"seed\": 22,\n",
    "        \"learning_rate\": 1e-3,\n",
    "    }    \n",
    "\n",
    "    if dataset == \"mnist\":\n",
    "        params[\"n_epochs\"] = n_epochs\n",
    "        params[\"batch_size\"] = 128\n",
    "\n",
    "        if training == \"node\":\n",
    "            params[\"odeint_method\"] = odeint_method\n",
    "            path, batch, loss, train_time = train_mnist_node(n_epochs, params[\"learning_rate\"], params[\"seed\"], odeint_method)\n",
    "\n",
    "        elif training == \"rnode\":\n",
    "            params[\"odeint_method\"] = odeint_method\n",
    "            params[\"lambda_k\"] = lambda_k\n",
    "            params[\"lambda_j\"] = lambda_j\n",
    "            \n",
    "            path, batch, loss, train_time = train_mnist_rnode(n_epochs, params[\"learning_rate\"], params[\"seed\"], lambda_k, lambda_j, odeint_method)\n",
    "\n",
    "        elif training == \"cfm\":\n",
    "            params[\"sigma\"] = sigma\n",
    "            path, batch, loss, train_time = train_mnist_cfm(n_epochs, params[\"learning_rate\"], params[\"seed\"], sigma)\n",
    "    else:\n",
    "        params[\"n_batches\"] = n_batches\n",
    "        params[\"batch_size\"] = batch_size\n",
    "\n",
    "        if training == \"node\":\n",
    "            params[\"odeint_method\"] = odeint_method\n",
    "            path, batch, loss, train_time = train_toy_node(n_batches, batch_size, dataset, params[\"learning_rate\"], params[\"seed\"], odeint_method)\n",
    "\n",
    "        elif training == \"rnode\":\n",
    "            params[\"odeint_method\"] = odeint_method\n",
    "            params[\"lambda_k\"] = lambda_k\n",
    "            params[\"lambda_j\"] = lambda_j\n",
    "            path, batch, loss, train_time = train_toy_rnode(n_batches, batch_size, dataset, params[\"learning_rate\"], params[\"seed\"], lambda_k, lambda_j, odeint_method)\n",
    "\n",
    "        elif training == \"cfm\":\n",
    "            params[\"sigma\"] = sigma\n",
    "            path, batch, loss, train_time = train_toy_cfm(n_batches, batch_size, dataset, params[\"learning_rate\"], params[\"seed\"], sigma)\n",
    "\n",
    "    save_train_logs(path, params, batch, loss, train_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from ContNF import train_mnist_cfm\\n%mprun -f train_mnist_cfm train_mnist_cfm(5)'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from ContNF import train_mnist_cfm\n",
    "%mprun -f train_mnist_cfm train_mnist_cfm(5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                      Test Folder                      '"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"                      Test Folder                      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILS\n",
    "\n",
    "def setup_model_and_data(dataset: str, seed: int=10, toy_samples: int=2000):\n",
    "    \"\"\"\n",
    "    for MNIST: returns batched samples from the normal distribution matching the test-loader\n",
    "        \n",
    "\n",
    "    for TOY: returns 2000 unsqueezed samples from both the normal and target distribution\n",
    "        x[0], y[0][0] to access samples 2000x2\n",
    "            \n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if dataset == \"mnist\":\n",
    "        y = test_loader\n",
    "        x = torch.randn((len(test_loader), 128, 1, 28, 28))\n",
    "        odefunc = UNet()\n",
    "    else:\n",
    "        y = sampler(dataset)(toy_samples)\n",
    "        x = torch.randn_like(y).unsqueeze(0)\n",
    "        y = [(y, 0)]\n",
    "        odefunc = MLP()\n",
    "    \n",
    "    return x, y, odefunc\n",
    "\n",
    "    \n",
    "def evaluate_model(path, x, y, odefunc):\n",
    "    odefunc.load_state_dict(torch.load(path))\n",
    "    odefunc.eval()\n",
    "\n",
    "    cont_NF = NODE(odefunc)\n",
    "    c, neg_log, length, w2d = 0, 0, 0, 0\n",
    "\n",
    "    for y_batched, _ in y:\n",
    "        x_batched = x[c]\n",
    "        c += 1\n",
    "\n",
    "        flow = cont_NF(x_batched, traj=True, t=torch.linspace(1,0,10))\n",
    "        neg_log += -cont_NF.log_likelihood(y_batched, 5)\n",
    "        length += flow_length(flow)\n",
    "        w2d += wasserstein2(flow[-1], y_batched)\n",
    "\n",
    "    return neg_log/c, length/c, w2d/c\n",
    "\n",
    "\n",
    "def save_test_logs(directory, training, name, neg_log, length, w2d):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    file_path = directory + '/test.txt'\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "\n",
    "        if \"mnist\" in directory:\n",
    "            file.write(f\"{training} for MNIST dataset\\n\")\n",
    "            headers = [\"epoch\", \"negative log-likelihood\", \"flow length\", \"wasserstein2 distance\"]\n",
    "        else:\n",
    "            file.write(f\"{training} for TOY dataset\\n\")\n",
    "            headers = [\"batch\", \"negative log-likelihood\", \"flow length\", \"wasserstein2 distance\"]\n",
    "            \n",
    "        data = list(zip(name, neg_log, length, w2d))\n",
    "        file.write(tabulate(data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "def extract_batch_number(filename):\n",
    "    return int(filename.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(directory: str):\n",
    "    dataset, training, _ = directory.split('/')\n",
    "    \n",
    "    x, y, odefunc = setup_model_and_data(dataset)\n",
    "    name, neg_log, length, w2d = [], [], [], []\n",
    "    sorted_files = sorted(os.listdir(directory), key=extract_batch_number)\n",
    "\n",
    "    for model in sorted_files:\n",
    "        model_path = os.path.join(directory, model)\n",
    "        a, b, c = evaluate_model(model_path, x, y, odefunc)\n",
    "        print(f\"    {model} evaluated\\n\")\n",
    "        name.append(model)\n",
    "        neg_log.append(a)\n",
    "        length.append(b)\n",
    "        w2d.append(c)\n",
    "\n",
    "    save_test_logs(os.path.join(dataset, training, 'logs'), training, name, neg_log, length, w2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOY PLOTS\n",
    "\n",
    "def set_plot_limits(dataset: str):\n",
    "    if dataset == 'moons':\n",
    "        return 6\n",
    "    elif dataset == 'spirals':\n",
    "        return 14\n",
    "    elif dataset == 'gaussians':\n",
    "        return 5\n",
    "\n",
    "def toy_density_estimation1(model_path:str, seed:int = 3):\n",
    "    dataset, training, _, name = model_path.split('/')\n",
    "    directory = os.path.join(dataset, training, 'plots')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    x, y, odefunc = setup_model_and_data(dataset, seed, 10000)\n",
    "    t=torch.linspace(1,0,4).type(torch.float32)\n",
    "    x = x[0]\n",
    "\n",
    "    odefunc.load_state_dict(torch.load(model_path))\n",
    "    odefunc.eval()\n",
    "\n",
    "    cont_NF = NODE(odefunc)\n",
    "    flow = cont_NF(x, traj=True, t=t)\n",
    "    limits = set_plot_limits(dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.set_facecolor('black')\n",
    "            ax.scatter(flow[i,:,0], flow[i,:,1], color='#F7F7F7', s=0.1)\n",
    "            ax.set_title(f't = {t[i]:.2f}')\n",
    "\n",
    "            ax.set_xlim(-limits, limits)  \n",
    "            ax.set_ylim(-limits, limits)\n",
    "\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(directory, f'{dataset}_density1'))\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def toy_density_estimation2(model_path: str, seed: int=3):\n",
    "    dataset, training, _, name = model_path.split('/')\n",
    "    directory = os.path.join(dataset, training, 'plots')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    x, y, odefunc = setup_model_and_data(dataset, seed, 20000)\n",
    "    t=torch.linspace(1,0,4).type(torch.float32)\n",
    "    x = x[0]\n",
    "\n",
    "    odefunc.load_state_dict(torch.load(model_path))\n",
    "    odefunc.eval()\n",
    "\n",
    "    cont_NF = NODE(odefunc)\n",
    "    flow = cont_NF(x, traj=True, t=t)\n",
    "    limits = set_plot_limits(dataset)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5), dpi=300)\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.set_facecolor('black')\n",
    "            sns.kdeplot(x=flow[i,:,0], y=flow[i,:,1], fill=True, thresh=0, levels=100, cmap='magma', ax=ax, bw_adjust=.3, clip=(limits,-limits))\n",
    "            ax.set_title(f't = {1-t[i]:.2f}')\n",
    "\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(directory, f'{dataset}_density2'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_toy_flow(model_path: str, seed: int=3):\n",
    "    dataset, training, _, name = model_path.split('/')\n",
    "    directory = os.path.join(dataset, training, 'plots')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    x, y, odefunc = setup_model_and_data(dataset, seed)\n",
    "    y = y[0][0]\n",
    "    t=torch.linspace(1,0,100**2).type(torch.float32)\n",
    "    limit = set_plot_limits(dataset)\n",
    "    Y, X = np.mgrid[-limit:limit:100j, -limit:limit:100j]\n",
    "\n",
    "    odefunc.load_state_dict(torch.load(model_path))\n",
    "    odefunc.eval()\n",
    "\n",
    "    a, b = X.flatten(), Y.flatten()\n",
    "    grid = torch.Tensor(np.array((a, b))).T\n",
    "    flow_velocity = odefunc(t, grid)\n",
    "\n",
    "    U, V = flow_velocity[:,0].view(100,100).detach().numpy() , flow_velocity[:,1].view(100,100).detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.gca()\n",
    "    ax.streamplot(x=X, y=Y, u=-U, v=-V, density=(8,6), color='firebrick', linewidth=.5, maxlength=.3, integration_direction='forward', start_points=x[0], arrowsize=.6)\n",
    "\n",
    "    ax.set_title(f'flow trained with {training}')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.savefig(os.path.join(directory, f'{dataset}_flow'))\n",
    "\n",
    "\n",
    "def plot_toy_flow2(model_path: str, samples: int=15,  seed: int=3):\n",
    "    dataset, training, _, name = model_path.split('/')\n",
    "    directory = os.path.join(dataset, training, 'plots')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    x, y, odefunc = setup_model_and_data(dataset, seed, samples)\n",
    "    t=torch.linspace(1,0,100).type(torch.float32)\n",
    "    color = np.linspace(0, 1, samples)\n",
    "    x = x[0]\n",
    "\n",
    "    odefunc.load_state_dict(torch.load(model_path))\n",
    "    odefunc.eval()\n",
    "\n",
    "    cont_NF = NODE(odefunc)\n",
    "    flow = cont_NF(x, traj=True, t=t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fig, ax = plt.subplots()\n",
    "        cmap = plt.get_cmap('magma')\n",
    "        ax.set_facecolor('black')\n",
    "        for i in range(samples):\n",
    "            ax.plot(flow[:,i,0], flow[:,i,1], color=cmap(color[i]), alpha=.7, zorder=0, label=\"flow\")           \n",
    "        ax.scatter(flow[0,:,0],flow[0,:,1],c=color, cmap=cmap, s=5, label=\"prior sample\", zorder=1)                     #c=\"royalblue\"\n",
    "        ax.scatter(flow[-1,:,0], flow[-1,:,1],c=color, cmap=cmap, s=5, label=\"gen sample\", zorder=1)  #c=\"darkorange\"\n",
    "\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "    ax.set_title(f'flow trained with {training}')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.savefig(os.path.join(directory, f'{dataset}_flow2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_toy_flow2(\"moons/node/models/18000_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy_density_estimation1(\"gaussians/rnode/models/50000_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy_density_estimation2(\"moons/node/models/18000_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST PLOTS\n",
    "\n",
    "def generate_grid(model_path: str, seed: int=4):\n",
    "    dataset, training, _, name = model_path.split('/')\n",
    "    directory = os.path.join(dataset, training, 'plots')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    x, _, odefunc = setup_model_and_data(dataset, seed)\n",
    "    x = x[0, :25, :, :]\n",
    "\n",
    "    t=torch.linspace(0,1,2).type(torch.float32)\n",
    "\n",
    "    odefunc.load_state_dict(torch.load(model_path))\n",
    "    odefunc.eval()\n",
    "\n",
    "    cont_NF = NODE(odefunc)\n",
    "    images = cont_NF(torch.randn((25,1,28,28)), traj=False, t=t).detach().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 10))  \n",
    "    axes = axes.flatten()  \n",
    "\n",
    "    for i in range(25):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i, 0, :, :], cmap='gray')  \n",
    "        ax.axis('off')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(directory, f'{dataset}_grid'))\n",
    "\n",
    "\n",
    "def plot_mnist_flow(model_path: str, seed: int=4):\n",
    "    dataset, training, _, name = model_path.split('/')\n",
    "    directory = os.path.join(dataset, training, 'plots')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    x, _, odefunc = setup_model_and_data(dataset, seed)\n",
    "    x = x[0, :1, :, :]\n",
    "\n",
    "    t=torch.linspace(0,1,5).type(torch.float32)\n",
    "\n",
    "    odefunc.load_state_dict(torch.load(model_path))\n",
    "    odefunc.eval()\n",
    "\n",
    "    cont_NF = NODE(odefunc)\n",
    "    flow = cont_NF(x, traj=True, t=t).detach().numpy()\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(flow[i, 0, 0, :, :], cmap='gray')\n",
    "        ax.set_title(f't = {t[i]:.2f}')\n",
    "        ax.axis('off')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(directory, f'{dataset}_flow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_mnist_flow('mnist/cfm/models/3_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_grid('mnist/cfm/models/3_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Playground\\n\\ntoy = torch.randint(low=0, high=10, size=(3,2)).to(torch.float32)\\nmnist = torch.randint(low=0, high=10, size=(3,1,28,28)).to(torch.float32)\\n\\nt=torch.linspace(0, 1, 10).type(torch.float32)\\n\\nNF_toy = NODE(MLP())\\nNF_mnist = NODE(UNet())\\n\\ntraj_toy = NF_toy(toy, traj=True, t=t)\\ntraj_mnist = NF_mnist(mnist, traj=True, t=t)\\n'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Playground\n",
    "\n",
    "toy = torch.randint(low=0, high=10, size=(3,2)).to(torch.float32)\n",
    "mnist = torch.randint(low=0, high=10, size=(3,1,28,28)).to(torch.float32)\n",
    "\n",
    "t=torch.linspace(0, 1, 10).type(torch.float32)\n",
    "\n",
    "NF_toy = NODE(MLP())\n",
    "NF_mnist = NODE(UNet())\n",
    "\n",
    "traj_toy = NF_toy(toy, traj=True, t=t)\n",
    "traj_mnist = NF_mnist(mnist, traj=True, t=t)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                          SCRIPTS                      '"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"                          SCRIPTS                      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_models('moons/rnode/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(dataset='moons', training='node', odeint_method='rk4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"datasets: ['moons', 'gaussians', 'spirals', 'mnist']\"\"\"\n",
    "\n",
    "#script to train with all training methods\n",
    "\n",
    "train_TOY = False\n",
    "train_MNIST = False\n",
    "\n",
    "training_methods = ['cfm', 'rnode', 'node']\n",
    "odeint_method = 'rk4'\n",
    "\n",
    "for i in training_methods:\n",
    "    #function with parameters for TOY dataset\n",
    "    if train_TOY:\n",
    "        train_model('gaussians', i, \n",
    "                    n_batches=20000, \n",
    "                    batch_size=200, \n",
    "                    odeint_method=odeint_method)\n",
    "\n",
    "    #function with parameters for MNIST dataset\n",
    "    if train_MNIST:\n",
    "        train_model('mnist', i, \n",
    "                    n_epochs=10, \n",
    "                    odeint_method=odeint_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to evaluate all different models w.r.t one dataset\n",
    "evaluate = False\n",
    "dataset = 'moons'\n",
    "\n",
    "if evaluate:\n",
    "    for i in training_methods:\n",
    "        print(f\"TEST {i} for {dataset}\\n\")\n",
    "        evaluate_models(os.path.join(dataset, i, 'models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to train mnist in 3-epoch intervalls\n",
    "\n",
    "#train_model('mnist', 'cfm', n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to download a directory\n",
    "\n",
    "#from google.colab import files\n",
    "def download_folder(folder_path: str):\n",
    "    output_path = folder_path + '_backup'\n",
    "    shutil.make_archive(output_path, 'zip', folder_path)\n",
    "    #files.download(output_path + '.zip')\n",
    "\n",
    "#download_folder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
