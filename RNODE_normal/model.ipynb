{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint_adjoint as odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Loader and Generator\"\"\"\n",
    "\n",
    "def data_loader(num_batches = 10, batch_size = 64, train_test_split = .8, data_mean = 5, data_variance = 5):\n",
    "    target_distr = torch.distributions.MultivariateNormal(data_mean*torch.ones(1),data_variance*torch.eye(1))\n",
    "    data = list()\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        batch = target_distr.sample()\n",
    "        for __ in range(batch_size-1):\n",
    "            batch = torch.cat((batch, target_distr.sample()), 0)\n",
    "        data.append(batch.unsqueeze(1))\n",
    "\n",
    "    split = int(train_test_split*len(data))\n",
    "\n",
    "    return data[:split], data[split:]\n",
    "\n",
    "\n",
    "def generate(num_samples, device):\n",
    "    model = torch.load(\"trained_model.pth\")\n",
    "    with torch.no_grad():\n",
    "        z0 = torch.randn(num_samples, 1).to(device)\n",
    "        t_reverse = torch.linspace(1, 0, steps=5).to(device)\n",
    "        generated_points = model(z0, t_reverse)\n",
    "\n",
    "    show(generated_points)\n",
    "\n",
    "\n",
    "def on_key(event):\n",
    "    if event.key == 'q':\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def show(datapoints, data_mean, data_variance):\n",
    "    plt.hist(datapoints.T, bins=50, density=True, color='blue', alpha=0.7)\n",
    "    plt.title('Histogram')\n",
    "    plt.grid(False)\n",
    "\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = 1 / (data_variance*np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((x-data_mean)/data_variance)**2)  \n",
    "    plt.plot(x, y, color='red', label='Normal Gaussian')\n",
    "    \n",
    "    plt.gcf().canvas.mpl_connect('key_press_event', on_key)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RNODE model for 1D normal distribution\"\"\"\n",
    "\n",
    "class ODEfunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEfunc, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, t, z):\n",
    "        out = self.linear1(z)\n",
    "        out = self.softplus(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.softplus(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.softplus(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CNF, self).__init__()\n",
    "        self.odefunc = ODEfunc(dim)\n",
    "        self.distr = torch.distributions.MultivariateNormal(torch.ones(dim),torch.eye(dim))\n",
    "        self.lambda_k = 0.01\n",
    "        self.lambda_j = 0.01\n",
    "        \n",
    "    def forward(self, t, states):\n",
    "        z = states[0]       #dynamics f\n",
    "        logp_z = states[1]  #log-det of the Jacobian\n",
    "        E = states[2]       #kinetic Energy\n",
    "        n = states[3]       #Frobenius norm of the Jacobian\n",
    "        batchsize = z.shape[0]\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #z.requires_grad = True\n",
    "\n",
    "            dz_dt, dlogp_z_dt = vjp(self.odefunc, (t,z))\n",
    "            dlogp_z_dt =  dlogp_z_dt[1]\n",
    "\n",
    "            dE_dt = (torch.linalg.vector_norm(dz_dt, dim=1)**2).unsqueeze(1)\n",
    "            dn_dt = (torch.linalg.vector_norm(dlogp_z_dt, dim=1)**2).unsqueeze(1)\n",
    "\n",
    "            return (dz_dt, dlogp_z_dt, dE_dt, dn_dt)\n",
    "\n",
    "\n",
    "def vjp(f, z):\n",
    "    return torch.autograd.functional.vjp(f, z, v=torch.randn_like(z[1]), create_graph=True, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parameters and Initialization\"\"\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_batches = 10\n",
    "batch_size = 1\n",
    "train_test_split = .8\n",
    "data_mean = 5\n",
    "data_variance = 5\n",
    "\n",
    "train_data, test_data = data_loader(num_batches, batch_size, train_test_split, data_mean, data_variance)\n",
    "\n",
    "model = CNF(1).to(device)\n",
    "t0 = 0\n",
    "t1 = 1\n",
    "q0 = torch.distributions.MultivariateNormal(torch.zeros(1),torch.eye(1))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training the Model\"\"\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_data, desc=f'Epoch {epoch + 1}/{num_epochs}', total=num_batches)\n",
    "    for batchidx, x in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x.requires_grad = True\n",
    "        l0 = torch.zeros(x.size(), requires_grad=True)\n",
    "        E0 = torch.zeros(x.size(), requires_grad=True)\n",
    "        n0 = torch.zeros(x.size(), requires_grad=True)\n",
    "        initial_values = (x, l0, E0, n0)\n",
    "        print(\"start\")\n",
    "\n",
    "        z_t, logpz, E_t, n_t = odeint(model, initial_values, torch.tensor([t0, t1]).type(torch.float32).to(device))\n",
    "        z_t0, logp_t0, E_t0, n_t0 = z_t[-1], logpz[-1], E_t[-1], n_t[-1]\n",
    "\n",
    "        logp_x = q0.log_prob(z_t0).to(device) - logp_t0 - E_t0 - n_t0 \n",
    "        loss = -logp_x.mean(0)\n",
    "\n",
    "        print(\"loss\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"optimized\")\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "torch.save(model.state_dict(), \"rnode_normal.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
