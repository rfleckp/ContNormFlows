{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "#!pip install torchdiffeq\n",
    "#!pip install POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Various utilities for neural networks. From https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/nn.py\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\n",
    "class SiLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * th.sigmoid(x)\n",
    "\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "\n",
    "def conv_nd(dims, *args, **kwargs):\n",
    "    \"\"\"Create a 1D, 2D, or 3D convolution module.\"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.Conv1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.Conv2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.Conv3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "def linear(*args, **kwargs):\n",
    "    \"\"\"Create a linear module.\"\"\"\n",
    "    return nn.Linear(*args, **kwargs)\n",
    "\n",
    "\n",
    "def avg_pool_nd(dims, *args, **kwargs):\n",
    "    \"\"\"Create a 1D, 2D, or 3D average pooling module.\"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.AvgPool1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.AvgPool2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.AvgPool3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "def update_ema(target_params, source_params, rate=0.99):\n",
    "    \"\"\"Update target parameters to be closer to those of source parameters using an exponential\n",
    "    moving average.\n",
    "\n",
    "    :param target_params: the target parameter sequence.\n",
    "    :param source_params: the source parameter sequence.\n",
    "    :param rate: the EMA rate (closer to 1 means slower).\n",
    "    \"\"\"\n",
    "    for targ, src in zip(target_params, source_params):\n",
    "        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "def scale_module(module, scale):\n",
    "    \"\"\"Scale the parameters of a module and return it.\"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().mul_(scale)\n",
    "    return module\n",
    "\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"Take the mean over all non-batch dimensions.\"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "\n",
    "def normalization(channels):\n",
    "    \"\"\"Make a standard normalization layer.\n",
    "\n",
    "    :param channels: number of input channels.\n",
    "    :return: an nn.Module for normalization.\n",
    "    \"\"\"\n",
    "    return GroupNorm32(32, channels)\n",
    "\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element. These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = th.exp(\n",
    "        -math.log(max_period)\n",
    "        * th.arange(start=0, end=half, dtype=th.float32, device=timesteps.device)\n",
    "        / half\n",
    "    )\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def checkpoint(func, inputs, params, flag):\n",
    "    \"\"\"Evaluate a function without caching intermediate activations, allowing for reduced memory at\n",
    "    the expense of extra compute in the backward pass.\n",
    "\n",
    "    :param func: the function to evaluate.\n",
    "    :param inputs: the argument sequence to pass to `func`.\n",
    "    :param params: a sequence of parameters `func` depends on but does not\n",
    "                   explicitly take as arguments.\n",
    "    :param flag: if False, disable gradient checkpointing.\n",
    "    \"\"\"\n",
    "    if flag:\n",
    "        args = tuple(inputs) + tuple(params)\n",
    "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
    "    else:\n",
    "        return func(*inputs)\n",
    "\n",
    "\n",
    "class CheckpointFunction(th.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, run_function, length, *args):\n",
    "        ctx.run_function = run_function\n",
    "        ctx.input_tensors = list(args[:length])\n",
    "        ctx.input_params = list(args[length:])\n",
    "        #print(type(ctx.input_tensors))\n",
    "        if type(ctx.input_tensors) == None:\n",
    "            print(type(args))\n",
    "        #ctx.save_for_backward(*ctx.input_tensors) modificado por mi\n",
    "        with th.no_grad():\n",
    "            output_tensors = ctx.run_function(*ctx.input_tensors)\n",
    "        return output_tensors\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *output_grads):\n",
    "        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n",
    "        with th.enable_grad():\n",
    "            # Fixes a bug where the first op in run_function modifies the\n",
    "            # Tensor storage in place, which is not allowed for detach()'d\n",
    "            # Tensors.\n",
    "            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n",
    "            output_tensors = ctx.run_function(*shallow_copies)\n",
    "        input_grads = th.autograd.grad(\n",
    "            output_tensors,\n",
    "            ctx.input_tensors + ctx.input_params,\n",
    "            output_grads,\n",
    "            allow_unused=True,\n",
    "        )\n",
    "        del ctx.input_tensors\n",
    "        del ctx.input_params\n",
    "        del output_tensors\n",
    "        return (None, None) + input_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"From https://raw.githubusercontent.com/openai/guided-diffusion/main/guided_diffusion/unet.py.\"\"\"\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def convert_module_to_f16(l):\n",
    "    \"\"\"Convert primitive modules to float16.\"\"\"\n",
    "    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n",
    "        l.weight.data = l.weight.data.half()\n",
    "        if l.bias is not None:\n",
    "            l.bias.data = l.bias.data.half()\n",
    "\n",
    "\n",
    "def convert_module_to_f32(l):\n",
    "    \"\"\"Convert primitive modules to float32, undoing convert_module_to_f16().\"\"\"\n",
    "    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n",
    "        l.weight.data = l.weight.data.float()\n",
    "        if l.bias is not None:\n",
    "            l.bias.data = l.bias.data.float()\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    \"\"\"Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spacial_dim: int,\n",
    "        embed_dim: int,\n",
    "        num_heads_channels: int,\n",
    "        output_dim: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            th.randn(embed_dim, spacial_dim**2 + 1) / embed_dim**0.5\n",
    "        )\n",
    "        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n",
    "        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n",
    "        self.num_heads = embed_dim // num_heads_channels\n",
    "        self.attention = QKVAttention(self.num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, *_spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)  # NC(HW)\n",
    "        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n",
    "        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n",
    "        x = self.qkv_proj(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x[:, :, 0]\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"Any module where forward() takes timestep embeddings as a second argument.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"Apply the module to `x` given `emb` timestep embeddings.\"\"\"\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"A sequential module that passes timestep embeddings to the children that support it as an\n",
    "    extra input.\"\"\"\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"An upsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then upsampling occurs in the\n",
    "        inner-two dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.dims = dims\n",
    "        if use_conv:\n",
    "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.dims == 3:\n",
    "            x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"A downsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then downsampling occurs in the\n",
    "        inner-two dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.dims = dims\n",
    "        stride = 2 if dims != 3 else (1, 2, 2)\n",
    "        if use_conv:\n",
    "            self.op = conv_nd(dims, self.channels, self.out_channels, 3, stride=stride, padding=1)\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):\n",
    "    \"\"\"A residual block that can optionally change the number of channels.\n",
    "\n",
    "    :param channels: the number of input channels.\n",
    "    :param emb_channels: the number of timestep embedding channels.\n",
    "    :param dropout: the rate of dropout.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    :param use_conv: if True and out_channels is specified, use a spatial convolution instead of a\n",
    "        smaller 1x1 convolution to change the channels in the skip connection.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param use_checkpoint: if True, use gradient checkpointing on this module.\n",
    "    :param up: if True, use this block for upsampling.\n",
    "    :param down: if True, use this block for downsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        dims=2,\n",
    "        use_checkpoint=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            normalization(channels),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = Upsample(channels, False, dims)\n",
    "            self.x_upd = Upsample(channels, False, dims)\n",
    "        elif down:\n",
    "            self.h_upd = Downsample(channels, False, dims)\n",
    "            self.x_upd = Downsample(channels, False, dims)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            normalization(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 3, padding=1)\n",
    "        else:\n",
    "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"Apply the block to a Tensor, conditioned on a timestep embedding.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of features.\n",
    "        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        return checkpoint(self._forward, (x, emb), self.parameters(), self.use_checkpoint)\n",
    "\n",
    "    def _forward(self, x, emb):\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = th.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "        return self.skip_connection(x) + h\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"An attention block that allows spatial positions to attend to each other.\n",
    "\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert (\n",
    "                channels % num_head_channels == 0\n",
    "            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = normalization(channels)\n",
    "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
    "        if use_new_attention_order:\n",
    "            # split qkv before split heads\n",
    "            self.attention = QKVAttention(self.num_heads)\n",
    "        else:\n",
    "            # split heads before split qkv\n",
    "            self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self._forward, (x,), self.parameters(), True)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"A counter for the `thop` package to count the operations in an attention operation.\n",
    "\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"A module which performs QKV attention.\n",
    "\n",
    "    Matches legacy QKVAttention + input/output heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    \"\"\"A module which performs QKV attention and splits in a different order.\"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\",\n",
    "            (q * scale).view(bs * self.n_heads, ch, length),\n",
    "            (k * scale).view(bs * self.n_heads, ch, length),\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    \"\"\"The full UNet model with attention and timestep embedding.\n",
    "\n",
    "    :param in_channels: channels in the input Tensor.\n",
    "    :param model_channels: base channel count for the model.\n",
    "    :param out_channels: channels in the output Tensor.\n",
    "    :param num_res_blocks: number of residual blocks per downsample.\n",
    "    :param attention_resolutions: a collection of downsample rates at which\n",
    "        attention will take place. May be a set, list, or tuple.\n",
    "        For example, if this contains 4, then at 4x downsampling, attention\n",
    "        will be used.\n",
    "    :param dropout: the dropout probability.\n",
    "    :param channel_mult: channel multiplier for each level of the UNet.\n",
    "    :param conv_resample: if True, use learned convolutions for upsampling and\n",
    "        downsampling.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param num_classes: if specified (as an int), then this model will be\n",
    "        class-conditional with `num_classes` classes.\n",
    "    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n",
    "    :param num_heads: the number of attention heads in each attention layer.\n",
    "    :param num_heads_channels: if specified, ignore num_heads and instead use\n",
    "                               a fixed channel width per attention head.\n",
    "    :param num_heads_upsample: works with num_heads to set a different number\n",
    "                               of heads for upsampling. Deprecated.\n",
    "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
    "    :param resblock_updown: use residual blocks for up/downsampling.\n",
    "    :param use_new_attention_order: use a different attention pattern for potentially\n",
    "                                    increased efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_classes = num_classes\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
    "\n",
    "        ch = input_ch = int(channel_mult[0] * model_channels)\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
    "        )\n",
    "        self._feature_size = ch\n",
    "        input_block_chans = [ch]\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=int(mult * model_channels),\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = int(mult * model_channels)\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=num_head_channels,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch + ich,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=int(model_channels * mult),\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = int(model_channels * mult)\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads_upsample,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                if level and i == num_res_blocks:\n",
    "                    out_ch = ch\n",
    "                    layers.append(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            up=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            normalization(ch),\n",
    "            nn.SiLU(),\n",
    "            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"Convert the torso of the model to float16.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "        self.output_blocks.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"Convert the torso of the model to float32.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "        self.output_blocks.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, t, x, y=None):\n",
    "        \"\"\"Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param y: an [N] Tensor of labels, if class-conditional.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        timesteps = t\n",
    "        assert (y is not None) == (\n",
    "            self.num_classes is not None\n",
    "        ), \"must specify y if and only if the model is class-conditional\"\n",
    "        while timesteps.dim() > 1:\n",
    "            print(timesteps.shape)\n",
    "            timesteps = timesteps[:, 0]\n",
    "        if timesteps.dim() == 0:\n",
    "            timesteps = timesteps.repeat(x.shape[0])\n",
    "\n",
    "        hs = []\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            assert y.shape == (x.shape[0],)\n",
    "            emb = emb + self.label_emb(y)\n",
    "\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h, emb)\n",
    "        for module in self.output_blocks:\n",
    "            h = th.cat([h, hs.pop()], dim=1)\n",
    "            h = module(h, emb)\n",
    "        h = h.type(x.dtype)\n",
    "        return self.out(h)\n",
    "\n",
    "\n",
    "class SuperResModel(UNetModel):\n",
    "    \"\"\"A UNetModel that performs super-resolution.\n",
    "\n",
    "    Expects an extra kwarg `low_res` to condition on a low-resolution image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, in_channels, *args, **kwargs):\n",
    "        super().__init__(image_size, in_channels * 2, *args, **kwargs)\n",
    "\n",
    "    def forward(self, x, timesteps, low_res=None, **kwargs):\n",
    "        _, _, new_height, new_width = x.shape\n",
    "        upsampled = F.interpolate(low_res, (new_height, new_width), mode=\"bilinear\")\n",
    "        x = th.cat([x, upsampled], dim=1)\n",
    "        return super().forward(x, timesteps, **kwargs)\n",
    "\n",
    "\n",
    "class EncoderUNetModel(nn.Module):\n",
    "    \"\"\"The half UNet model with attention and timestep embedding.\n",
    "\n",
    "    For usage, see UNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        pool=\"adaptive\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        ch = int(channel_mult[0] * model_channels)\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n",
    "        )\n",
    "        self._feature_size = ch\n",
    "        input_block_chans = [ch]\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=int(mult * model_channels),\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = int(mult * model_channels)\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else Downsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=num_head_channels,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "        self.pool = pool\n",
    "        if pool == \"adaptive\":\n",
    "            self.out = nn.Sequential(\n",
    "                normalization(ch),\n",
    "                nn.SiLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                zero_module(conv_nd(dims, ch, out_channels, 1)),\n",
    "                nn.Flatten(),\n",
    "            )\n",
    "        elif pool == \"attention\":\n",
    "            assert num_head_channels != -1\n",
    "            self.out = nn.Sequential(\n",
    "                normalization(ch),\n",
    "                nn.SiLU(),\n",
    "                AttentionPool2d((image_size // ds), ch, num_head_channels, out_channels),\n",
    "            )\n",
    "        elif pool == \"spatial\":\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(self._feature_size, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2048, self.out_channels),\n",
    "            )\n",
    "        elif pool == \"spatial_v2\":\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(self._feature_size, 2048),\n",
    "                normalization(2048),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(2048, self.out_channels),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unexpected {pool} pooling\")\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"Convert the torso of the model to float16.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"Convert the torso of the model to float32.\"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x K] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        results = []\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb)\n",
    "            if self.pool.startswith(\"spatial\"):\n",
    "                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
    "        h = self.middle_block(h, emb)\n",
    "        if self.pool.startswith(\"spatial\"):\n",
    "            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
    "            h = th.cat(results, axis=-1)\n",
    "            return self.out(h)\n",
    "        else:\n",
    "            h = h.type(x.dtype)\n",
    "            return self.out(h)\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "class UNetModelWrapper(UNetModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_channels,\n",
    "        num_res_blocks,\n",
    "        channel_mult=None,\n",
    "        learn_sigma=False,\n",
    "        class_cond=False,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        use_checkpoint=False,\n",
    "        attention_resolutions=\"16\",\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        dropout=0,\n",
    "        resblock_updown=False,\n",
    "        use_fp16=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        \"\"\"Dim (tuple): (C, H, W)\"\"\"\n",
    "        image_size = dim[-1]\n",
    "        if channel_mult is None:\n",
    "            if image_size == 512:\n",
    "                channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n",
    "            elif image_size == 256:\n",
    "                channel_mult = (1, 1, 2, 2, 4, 4)\n",
    "            elif image_size == 128:\n",
    "                channel_mult = (1, 1, 2, 3, 4)\n",
    "            elif image_size == 64:\n",
    "                channel_mult = (1, 2, 3, 4)\n",
    "            elif image_size == 32:\n",
    "                channel_mult = (1, 2, 2, 2)\n",
    "            elif image_size == 28:\n",
    "                channel_mult = (1, 2, 2)\n",
    "            else:\n",
    "                raise ValueError(f\"unsupported image size: {image_size}\")\n",
    "        else:\n",
    "            channel_mult = list(channel_mult)\n",
    "\n",
    "        attention_ds = []\n",
    "        for res in attention_resolutions.split(\",\"):\n",
    "            attention_ds.append(image_size // int(res))\n",
    "\n",
    "        return super().__init__(\n",
    "            image_size=image_size,\n",
    "            in_channels=dim[0],\n",
    "            model_channels=num_channels,\n",
    "            out_channels=(dim[0] if not learn_sigma else dim[0] * 2),\n",
    "            num_res_blocks=num_res_blocks,\n",
    "            attention_resolutions=tuple(attention_ds),\n",
    "            dropout=dropout,\n",
    "            channel_mult=channel_mult,\n",
    "            num_classes=(num_classes if class_cond else None),\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            use_fp16=use_fp16,\n",
    "            num_heads=num_heads,\n",
    "            num_head_channels=num_head_channels,\n",
    "            num_heads_upsample=num_heads_upsample,\n",
    "            use_scale_shift_norm=use_scale_shift_norm,\n",
    "            resblock_updown=resblock_updown,\n",
    "            use_new_attention_order=use_new_attention_order,\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x, y=None, *args, **kwargs):\n",
    "        return super().forward(t, x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"From https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/optimal_transport.py\"\"\"\n",
    "import math\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import ot as pot\n",
    "import torch\n",
    "\n",
    "\n",
    "class OTPlanSampler:\n",
    "    \"\"\"OTPlanSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean\n",
    "    cost) with different implementations of the plan calculation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: str,\n",
    "        reg: float = 0.05,\n",
    "        reg_m: float = 1.0,\n",
    "        normalize_cost: bool = False,\n",
    "        warn: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the OTPlanSampler class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method: str\n",
    "            choose which optimal transport solver you would like to use.\n",
    "            Currently supported are [\"exact\", \"sinkhorn\", \"unbalanced\",\n",
    "            \"partial\"] OT solvers.\n",
    "        reg: float, optional\n",
    "            regularization parameter to use for Sinkhorn-based iterative solvers.\n",
    "        reg_m: float, optional\n",
    "            regularization weight for unbalanced Sinkhorn-knopp solver.\n",
    "        normalize_cost: bool, optional\n",
    "            normalizes the cost matrix so that the maximum cost is 1. Helps\n",
    "            stabilize Sinkhorn-based solvers. Should not be used in the vast\n",
    "            majority of cases.\n",
    "        warn: bool, optional\n",
    "            if True, raises a warning if the algorithm does not converge\n",
    "        \"\"\"\n",
    "        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n",
    "        # M is a cost matrix\n",
    "        if method == \"exact\":\n",
    "            self.ot_fn = pot.emd\n",
    "        elif method == \"sinkhorn\":\n",
    "            self.ot_fn = partial(pot.sinkhorn, reg=reg)\n",
    "        elif method == \"unbalanced\":\n",
    "            self.ot_fn = partial(pot.unbalanced.sinkhorn_knopp_unbalanced, reg=reg, reg_m=reg_m)\n",
    "        elif method == \"partial\":\n",
    "            self.ot_fn = partial(pot.partial.entropic_partial_wasserstein, reg=reg)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        self.reg = reg\n",
    "        self.reg_m = reg_m\n",
    "        self.normalize_cost = normalize_cost\n",
    "        self.warn = warn\n",
    "\n",
    "    def get_map(self, x0, x1):\n",
    "        \"\"\"Compute the OT plan (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : numpy array, shape (bs, bs)\n",
    "            represents the OT plan between minibatches\n",
    "        \"\"\"\n",
    "        a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n",
    "        if x0.dim() > 2:\n",
    "            x0 = x0.reshape(x0.shape[0], -1)\n",
    "        if x1.dim() > 2:\n",
    "            x1 = x1.reshape(x1.shape[0], -1)\n",
    "        x1 = x1.reshape(x1.shape[0], -1)\n",
    "        M = torch.cdist(x0, x1) ** 2\n",
    "        if self.normalize_cost:\n",
    "            M = M / M.max()  # should not be normalized when using minibatches\n",
    "        p = self.ot_fn(a, b, M.detach().cpu().numpy())\n",
    "        if not np.all(np.isfinite(p)):\n",
    "            print(\"ERROR: p is not finite\")\n",
    "            print(p)\n",
    "            print(\"Cost mean, max\", M.mean(), M.max())\n",
    "            print(x0, x1)\n",
    "        if np.abs(p.sum()) < 1e-8:\n",
    "            if self.warn:\n",
    "                warnings.warn(\"Numerical errors in OT plan, reverting to uniform plan.\")\n",
    "            p = np.ones_like(p) / p.size\n",
    "        return p\n",
    "\n",
    "    def sample_map(self, pi, batch_size, replace=True):\n",
    "        r\"\"\"Draw source and target samples from pi  $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : numpy array, shape (bs, bs)\n",
    "            represents the source minibatch\n",
    "        batch_size : int\n",
    "            represents the OT plan between minibatches\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (i_s, i_j) : tuple of numpy arrays, shape (bs, bs)\n",
    "            represents the indices of source and target data samples from $\\pi$\n",
    "        \"\"\"\n",
    "        p = pi.flatten()\n",
    "        p = p / p.sum()\n",
    "        choices = np.random.choice(\n",
    "            pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=replace\n",
    "        )\n",
    "        return np.divmod(choices, pi.shape[1])\n",
    "\n",
    "    def sample_plan(self, x0, x1, replace=True):\n",
    "        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        x1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        \"\"\"\n",
    "        pi = self.get_map(x0, x1)\n",
    "        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n",
    "        return x0[i], x1[j]\n",
    "\n",
    "    def sample_plan_with_labels(self, x0, x1, y0=None, y1=None, replace=True):\n",
    "        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch and draw source and target labeled samples from pi $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        y0 : Tensor, shape (bs)\n",
    "            represents the source label minibatch\n",
    "        y1 : Tensor, shape (bs)\n",
    "            represents the target label minibatch\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        x1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch drawn from $\\pi$\n",
    "        y0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source label minibatch drawn from $\\pi$\n",
    "        y1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the target label minibatch drawn from $\\pi$\n",
    "        \"\"\"\n",
    "        pi = self.get_map(x0, x1)\n",
    "        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n",
    "        return (\n",
    "            x0[i],\n",
    "            x1[j],\n",
    "            y0[i] if y0 is not None else None,\n",
    "            y1[j] if y1 is not None else None,\n",
    "        )\n",
    "\n",
    "    def sample_trajectory(self, X):\n",
    "        \"\"\"Compute the OT trajectories between different sample populations moving from the source\n",
    "        to the target distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Tensor, (bs, times, *dim)\n",
    "            different populations of samples moving from the source to the target distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        to_return : Tensor, (bs, times, *dim)\n",
    "            represents the OT sampled trajectories over time.\n",
    "        \"\"\"\n",
    "        times = X.shape[1]\n",
    "        pis = []\n",
    "        for t in range(times - 1):\n",
    "            pis.append(self.get_map(X[:, t], X[:, t + 1]))\n",
    "\n",
    "        indices = [np.arange(X.shape[0])]\n",
    "        for pi in pis:\n",
    "            j = []\n",
    "            for i in indices[-1]:\n",
    "                j.append(np.random.choice(pi.shape[1], p=pi[i] / pi[i].sum()))\n",
    "            indices.append(np.array(j))\n",
    "\n",
    "        to_return = []\n",
    "        for t in range(times):\n",
    "            to_return.append(X[:, t][indices[t]])\n",
    "        to_return = np.stack(to_return, axis=1)\n",
    "        return to_return\n",
    "\n",
    "\n",
    "def wasserstein(\n",
    "    x0: torch.Tensor,\n",
    "    x1: torch.Tensor,\n",
    "    method: Optional[str] = None,\n",
    "    reg: float = 0.05,\n",
    "    power: int = 2,\n",
    "    **kwargs,\n",
    ") -> float:\n",
    "    \"\"\"Compute the Wasserstein (1 or 2) distance (wrt Euclidean cost) between a source and a target\n",
    "    distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x0 : Tensor, shape (bs, *dim)\n",
    "        represents the source minibatch\n",
    "    x1 : Tensor, shape (bs, *dim)\n",
    "        represents the source minibatch\n",
    "    method : str (default : None)\n",
    "        Use exact Wasserstein or an entropic regularization\n",
    "    reg : float (default : 0.05)\n",
    "        Entropic regularization coefficients\n",
    "    power : int (default : 2)\n",
    "        power of the Wasserstein distance (1 or 2)\n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "        Wasserstein distance\n",
    "    \"\"\"\n",
    "    assert power == 1 or power == 2\n",
    "    # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n",
    "    # M is a cost matrix\n",
    "    if method == \"exact\" or method is None:\n",
    "        ot_fn = pot.emd2\n",
    "    elif method == \"sinkhorn\":\n",
    "        ot_fn = partial(pot.sinkhorn2, reg=reg)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n",
    "    if x0.dim() > 2:\n",
    "        x0 = x0.reshape(x0.shape[0], -1)\n",
    "    if x1.dim() > 2:\n",
    "        x1 = x1.reshape(x1.shape[0], -1)\n",
    "    M = torch.cdist(x0, x1)\n",
    "    if power == 2:\n",
    "        M = M**2\n",
    "    ret = ot_fn(a, b, M.detach().cpu().numpy(), numItermax=int(1e7))\n",
    "    if power == 2:\n",
    "        ret = math.sqrt(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements Conditional Flow Matcher Losses. From https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/conditional_flow_matching.py\"\"\"\n",
    "\n",
    "# Author: Alex Tong\n",
    "#         Kilian Fatras\n",
    "#         +++\n",
    "# License: MIT License\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def pad_t_like_x(t, x):\n",
    "    \"\"\"Function to reshape the time vector t by the number of dimensions of x.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor, shape (bs, *dim)\n",
    "        represents the source minibatch\n",
    "    t : FloatTensor, shape (bs)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t : Tensor, shape (bs, number of x dimensions)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    x: Tensor (bs, C, W, H)\n",
    "    t: Vector (bs)\n",
    "    pad_t_like_x(t, x): Tensor (bs, 1, 1, 1)\n",
    "    \"\"\"\n",
    "    if isinstance(t, (float, int)):\n",
    "        return t\n",
    "    return t.reshape(-1, *([1] * (x.dim() - 1)))\n",
    "\n",
    "\n",
    "class ConditionalFlowMatcher:\n",
    "    \"\"\"Base class for conditional flow matching methods. This class implements the independent\n",
    "    conditional flow matching methods from [1] and serves as a parent class for all other flow\n",
    "    matching methods.\n",
    "\n",
    "    It implements:\n",
    "    - Drawing data from gaussian probability path N(t * x1 + (1 - t) * x0, sigma) function\n",
    "    - conditional flow matching ut(x1|x0) = x1 - x0\n",
    "    - score function $\\nabla log p_t(x|x0, x1)$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Union[float, int] = 0.0):\n",
    "        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma : Union[float, int]\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def compute_mu_t(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Compute the mean of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mean mu_t: t * x1 + (1 - t) * x0\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        t = pad_t_like_x(t, x0)\n",
    "        return t * x1 + (1 - t) * x0\n",
    "\n",
    "    def compute_sigma_t(self, t):\n",
    "        \"\"\"\n",
    "        Compute the standard deviation of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        standard deviation sigma\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        del t\n",
    "        return self.sigma\n",
    "\n",
    "    def sample_xt(self, x0, x1, t, epsilon):\n",
    "        \"\"\"\n",
    "        Draw a sample from the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "        epsilon : Tensor, shape (bs, *dim)\n",
    "            noise sample from N(0, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        mu_t = self.compute_mu_t(x0, x1, t)\n",
    "        sigma_t = self.compute_sigma_t(t)\n",
    "        sigma_t = pad_t_like_x(sigma_t, x0)\n",
    "        return mu_t + sigma_t * epsilon\n",
    "\n",
    "    def compute_conditional_flow(self, x0, x1, t, xt):\n",
    "        \"\"\"\n",
    "        Compute the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        del t, xt\n",
    "        return x1 - x0\n",
    "\n",
    "    def sample_noise_like(self, x):\n",
    "        return torch.randn_like(x)\n",
    "\n",
    "    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n",
    "        \"\"\"\n",
    "        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n",
    "        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        (optionally) t : Tensor, shape (bs)\n",
    "            represents the time levels\n",
    "            if None, drawn from uniform [0,1]\n",
    "        return_noise : bool\n",
    "            return the noise sample epsilon\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "        (optionally) eps: Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        if t is None:\n",
    "            t = torch.rand(x0.shape[0]).type_as(x0)\n",
    "        assert len(t) == x0.shape[0], \"t has to have batch size dimension\"\n",
    "\n",
    "        eps = self.sample_noise_like(x0)\n",
    "        xt = self.sample_xt(x0, x1, t, eps)\n",
    "        ut = self.compute_conditional_flow(x0, x1, t, xt)\n",
    "        if return_noise:\n",
    "            return t, xt, ut, eps\n",
    "        else:\n",
    "            return t, xt, ut\n",
    "\n",
    "    def compute_lambda(self, t):\n",
    "        \"\"\"Compute the lambda function, see Eq.(23) [3].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lambda : score weighting function\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [4] Simulation-free Schrodinger bridges via score and flow matching, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        sigma_t = self.compute_sigma_t(t)\n",
    "        return 2 * sigma_t / (self.sigma**2 + 1e-8)\n",
    "\n",
    "\n",
    "class ExactOptimalTransportConditionalFlowMatcher(ConditionalFlowMatcher):\n",
    "    \"\"\"Child class for optimal transport conditional flow matching method. This class implements\n",
    "    the OT-CFM methods from [1] and inherits the ConditionalFlowMatcher parent class.\n",
    "\n",
    "    It overrides the sample_location_and_conditional_flow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Union[float, int] = 0.0):\n",
    "        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma : Union[float, int]\n",
    "        ot_sampler: exact OT method to draw couplings (x0, x1) (see Eq.(17) [1]).\n",
    "        \"\"\"\n",
    "        super().__init__(sigma)\n",
    "        self.ot_sampler = OTPlanSampler(method=\"exact\")\n",
    "\n",
    "    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n",
    "        r\"\"\"\n",
    "        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n",
    "        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n",
    "        with respect to the minibatch OT plan $\\Pi$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        (optionally) t : Tensor, shape (bs)\n",
    "            represents the time levels\n",
    "            if None, drawn from uniform [0,1]\n",
    "        return_noise : bool\n",
    "            return the noise sample epsilon\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        x0, x1 = self.ot_sampler.sample_plan(x0, x1)\n",
    "        return super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n",
    "\n",
    "    def guided_sample_location_and_conditional_flow(\n",
    "        self, x0, x1, y0=None, y1=None, t=None, return_noise=False\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n",
    "        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n",
    "        with respect to the minibatch OT plan $\\Pi$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        y0 : Tensor, shape (bs) (default: None)\n",
    "            represents the source label minibatch\n",
    "        y1 : Tensor, shape (bs) (default: None)\n",
    "            represents the target label minibatch\n",
    "        (optionally) t : Tensor, shape (bs)\n",
    "            represents the time levels\n",
    "            if None, drawn from uniform [0,1]\n",
    "        return_noise : bool\n",
    "            return the noise sample epsilon\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        x0, x1, y0, y1 = self.ot_sampler.sample_plan_with_labels(x0, x1, y0, y1)\n",
    "        if return_noise:\n",
    "            t, xt, ut, eps = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n",
    "            return t, xt, ut, y0, y1, eps\n",
    "        else:\n",
    "            t, xt, ut = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n",
    "            return t, xt, ut, y0, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot as ot\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchdiffeq import odeint_adjoint as odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 128\n",
    "n_epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0051, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = next(iter(train_loader))\n",
    "model = UNetModelWrapper(dim=(1, 28, 28), num_channels=32, num_res_blocks=1).to(device)\n",
    "\n",
    "t = torch.rand(img.shape[0]).type_as(img)\n",
    "print(img.shape)\n",
    "print(t.shape)\n",
    "loss = model(t, img) - torch.randn_like(img)\n",
    "\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dynamics, i.e., right hand side of the ODE\"\"\"\n",
    "class NODE(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.net = unet\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(t, x)\n",
    "    \n",
    "    \"\"\"plots the ode solutions from its initial value x at time 0 to the final solution at time 1 (t decides number of steps)\"\"\"\n",
    "    def plot_trajectories(self, trajectories=None, t=None, x=None):\n",
    "        fig, ax = plt.subplots()\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "\n",
    "            ax.plot(trajectories[:,:,0], trajectories[:,:,1], color=\"silver\", alpha=.7, zorder=0, label=\"flow\")            \n",
    "            ax.scatter(trajectories[0,:,0],trajectories[0,:,1], c=\"royalblue\", s=1, label=\"prior sample\", zorder=1)\n",
    "            ax.scatter(trajectories[-1,:,0], trajectories[-1,:,1], c=\"darkorange\", s=1, label=\"gen sample\", zorder=1)\n",
    "\n",
    "            handles, labels = plt.gca().get_legend_handles_labels()\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            plt.legend(by_label.values(), by_label.keys())\n",
    "        \n",
    "        return fig, ax\n",
    "    \n",
    "    \"\"\"log probability of the model with a normal gaussian as the initial distribution\"\"\"\n",
    "    def log_probability(self, t, x):\n",
    "        initial_distr = torch.distributions.MultivariateNormal(torch.zeros(28), torch.eye(28))\n",
    "\n",
    "        l0 = torch.zeros((x.size(0),1), requires_grad=False)\n",
    "        initial_values = (x, l0)\n",
    "\n",
    "        augmented_dynamics = Augmented_ODE(self)\n",
    "\n",
    "        z_t, log_det = odeint(augmented_dynamics, initial_values, t, method=\"rk4\")\n",
    "\n",
    "        logp_x = initial_distr.log_prob(z_t[-1]) + log_det[-1]\n",
    "\n",
    "        return -logp_x.mean()\n",
    "    \n",
    "    \"\"\"computes the average length of the trajectories from the initial values x to the values at the final timestep\"\"\"\n",
    "    def length_trajectories(self, trajectories=None, t=None, x=None):\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "            distances = torch.linalg.norm(trajectories[1:] - trajectories[:-1], dim=-1)\n",
    "            length = distances.sum(dim=0)\n",
    "\n",
    "            return length.mean()\n",
    "        \n",
    "    \"computes the Wasserstein2 distance between samples generated from the initial values x and samples y from the true distribution\"\n",
    "    def wasserstein2_distance(self, y, trajectories=None, t=None, x=None):\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "            z = trajectories[-1]\n",
    "            a, b = ot.unif(z.size(0)), ot.unif(y.size(0))\n",
    "            cost = torch.cdist(z, y) ** 2\n",
    "            distance = ot.emd2(a, b, cost.numpy())\n",
    "\n",
    "            return distance**.5\n",
    "\n",
    "    \n",
    "\"\"\"Augmented ODE for CNF without any regularization and choice of samples for the Hutchinson trace estimator and no gradient\"\"\"\n",
    "class Augmented_ODE(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        with torch.no_grad():\n",
    "            z = states[0]\n",
    "            dz_dt, dlogp_z_dt =  self.hutchinson_trace_estimator(t, z)\n",
    "            return (dz_dt, dlogp_z_dt)\n",
    "        \n",
    "    def hutchinson_trace_estimator(self, t, z, samples=20):\n",
    "        trace = 0\n",
    "        \n",
    "        for _ in range(samples):\n",
    "            epsilon = torch.randn_like(z)\n",
    "            output_f, vjp_f = torch.autograd.functional.vjp(self.odefunc, (t,z), v=epsilon, create_graph=False)\n",
    "            trace +=  (vjp_f[1]*epsilon).sum(1).unsqueeze(1)    \n",
    "\n",
    "        return output_f, trace/samples\n",
    "\n",
    "\"\"\"Augmented ODE to train RNODE\"\"\"\n",
    "class regularized_Augmented_ODE(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z = states[0]                                                               #dynamics \n",
    "            dz_dt, vjp_f, epsilon = self.vjp(self.odefunc, t, z)\n",
    "            dlog_det_dt =  (vjp_f*epsilon).sum(-1).sum(-1)                     #log-det of the Jacobian   \n",
    "            dE_dt = (torch.linalg.matrix_norm(dz_dt)**2)          #kinetic Energy\n",
    "            dn_dt = (torch.linalg.matrix_norm(vjp_f)**2)          #Frobenius norm of the Jacobian\n",
    "\n",
    "            \"\"\"            \n",
    "            print(\"dz_dt: \", dz_dt.shape)\n",
    "            print(\"dlog_det_dt: \", dlog_det_dt.shape)\n",
    "            print(\"dE_dt: \", dE_dt.shape)\n",
    "            print(\"dn_dt: \", dn_dt.shape)\n",
    "            \"\"\"\n",
    "            return (dz_dt, dlog_det_dt, dE_dt, dn_dt)\n",
    "\n",
    "    def vjp(self, f, t, z):\n",
    "        \"\"\"computes vector Jacobian product and returns (output of the function, vjp, epsilon)\"\"\"\n",
    "        epsilon = torch.randn_like(z)\n",
    "        output_f, vjp_f = torch.autograd.functional.vjp(f, (t,z), v=epsilon, create_graph=True)\n",
    "        return output_f, vjp_f[1], epsilon\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\"computes the loss of RNODE, given the dynamics 'model' and samples 'x'\"\"\"\n",
    "def rnode_loss(model: torch.nn.Module, x, lambda_k: float, lambda_j: float):\n",
    "    initial_distr = torch.distributions.MultivariateNormal(torch.zeros(28), torch.eye(28))\n",
    "    t = torch.linspace(0, 1, 5).type(torch.float32)\n",
    "    l0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    kin_E0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    n0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    initial_values = (x, l0, kin_E0, n0)\n",
    "\n",
    "    augmented_dynamics = regularized_Augmented_ODE(model)\n",
    "\n",
    "    z_t, log_det, E_t, n_t = odeint(augmented_dynamics, initial_values, t,\n",
    "                                    method=\"rk4\")\n",
    "\n",
    "    z1, l1, kin_E1, n1 = z_t[-1], log_det[-1], E_t[-1], n_t[-1]\n",
    "\n",
    "    logp_x = initial_distr.log_prob(z1) + l1 - lambda_k * kin_E1 - lambda_j * n1\n",
    "    loss = -logp_x.mean()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-3\n",
    "seed = 44\n",
    "epochs = 3\n",
    "lambda_k = .1\n",
    "lambda_j = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnode_training():\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model = UNetModelWrapper(dim=(1, 28, 28), num_channels=32, num_res_blocks=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "        num = 0\n",
    "        for i, (samples, labels) in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            #samples.requires_grad = True\n",
    "\n",
    "            x0 = samples\n",
    "            initial_distr = torch.distributions.MultivariateNormal(torch.zeros(28), torch.eye(28))\n",
    "            t = torch.linspace(0, 1, 5).type(torch.float32)\n",
    "            l0 = torch.zeros((x0.size(0),1), requires_grad=True)\n",
    "            kin_E0 = torch.zeros((x0.size(0),1), requires_grad=True)\n",
    "            n0 = torch.zeros((x0.size(0),1), requires_grad=True)\n",
    "            initial_values = (x0, l0, kin_E0, n0)\n",
    "\n",
    "            augmented_dynamics = regularized_Augmented_ODE(model)\n",
    "\n",
    "            z_t, log_det, E_t, n_t = odeint(augmented_dynamics, initial_values, t, method=\"rk4\")\n",
    "            print(\"yayayay\")\n",
    "            z1, l1, kin_E1, n1 = z_t[-1], log_det[-1], E_t[-1], n_t[-1]\n",
    "\n",
    "            logp_x = initial_distr.log_prob(z1) + l1 - lambda_k * kin_E1 - lambda_j * n1\n",
    "            loss = -logp_x.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            num += 1\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(\"mnist/rnode/models\", f\"{epoch}_model.pt\"))\n",
    "        print(f\"\\nloss: {loss/num}\")\n",
    "        \n",
    "\n",
    "    print(\"finished training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training loop for OT-CFM\"\"\"\n",
    "def cfm_training():\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    sigma = 0.0\n",
    "    model = UNetModelWrapper(dim=(1, 28, 28), num_channels=32, num_res_blocks=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    FM = ExactOptimalTransportConditionalFlowMatcher(sigma=sigma)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "        num = 0\n",
    "        for i, (samples, labels) in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x0 = torch.randn_like(samples)\n",
    "            t, xt, ut = FM.sample_location_and_conditional_flow(x0, samples)\n",
    "            vt = model(t, xt)\n",
    "            loss = torch.mean((vt - ut) ** 2)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            num += 1\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(\"mnist/rnode/models\", f\"{epoch}_model.pt\"))\n",
    "        print(f\"\\nloss: {loss/num}\")\n",
    "        \n",
    "\n",
    "    print(\"finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "yayayay\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/938 [00:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CheckpointFunctionBackward' object has no attribute 'input_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mrnode_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[250], line 39\u001b[0m, in \u001b[0;36mrnode_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m logp_x \u001b[38;5;241m=\u001b[39m initial_distr\u001b[38;5;241m.\u001b[39mlog_prob(z1) \u001b[38;5;241m+\u001b[39m l1 \u001b[38;5;241m-\u001b[39m lambda_k \u001b[38;5;241m*\u001b[39m kin_E1 \u001b[38;5;241m-\u001b[39m lambda_j \u001b[38;5;241m*\u001b[39m n1\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlogp_x\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/function.py:301\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/adjoint.py:134\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.backward\u001b[0;34m(ctx, *grad_y)\u001b[0m\n\u001b[1;32m    131\u001b[0m     time_vjps[i] \u001b[38;5;241m=\u001b[39m dLd_cur_t\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Run the augmented system backwards in time.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m aug_state \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_dynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maug_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_rtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_atol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_options\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m aug_state \u001b[38;5;241m=\u001b[39m [a[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m aug_state]  \u001b[38;5;66;03m# extract just the t[i - 1] value\u001b[39;00m\n\u001b[1;32m    140\u001b[0m aug_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m y[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# update to use our forward-pass estimate of the state\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/odeint.py:79\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     76\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/solvers.py:114\u001b[0m, in \u001b[0;36mFixedGridODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    112\u001b[0m dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mcallback_step(t0, y0, dt)\n\u001b[0;32m--> 114\u001b[0m dy, f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m dy\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;129;01mand\u001b[39;00m t1 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m t[j]:\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/fixed_grid.py:28\u001b[0m, in \u001b[0;36mRK4._step_func\u001b[0;34m(self, func, t0, dt, t1, y0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, t0, dt, t1, y0):\n\u001b[0;32m---> 28\u001b[0m     f0 \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEXT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rk4_alt_step_func(func, t0, dt, t1, y0, f0\u001b[38;5;241m=\u001b[39mf0, perturb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperturb), f0\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:197\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:165\u001b[0m, in \u001b[0;36m_ReverseFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmul \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:144\u001b[0m, in \u001b[0;36m_TupleFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 144\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_flat_to_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([f_\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f_ \u001b[38;5;129;01min\u001b[39;00m f])\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/adjoint.py:94\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.backward.<locals>.augmented_dynamics\u001b[0;34m(t, y_aug)\u001b[0m\n\u001b[1;32m     91\u001b[0m     _y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_strided(y, (), ())  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     _params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(torch\u001b[38;5;241m.\u001b[39mas_strided(param, (), ()) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m adjoint_params)  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     vjp_t, vjp_y, \u001b[38;5;241m*\u001b[39mvjp_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madjoint_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43madj_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# autograd.grad returns None if no gradient, set to zero.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m vjp_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(t) \u001b[38;5;28;01mif\u001b[39;00m vjp_t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m vjp_t\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    409\u001b[0m         grad_outputs_\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    425\u001b[0m     ):\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/function.py:301\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[240], line 141\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *output_grads)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, \u001b[38;5;241m*\u001b[39moutput_grads):\n\u001b[0;32m--> 141\u001b[0m     ctx\u001b[38;5;241m.\u001b[39minput_tensors \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tensors\u001b[49m]\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;66;03m# Fixes a bug where the first op in run_function modifies the\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# Tensor storage in place, which is not allowed for detach()'d\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# Tensors.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         shallow_copies \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mview_as(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_tensors]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CheckpointFunctionBackward' object has no attribute 'input_tensors'"
     ]
    }
   ],
   "source": [
    "loss = rnode_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor(38.6823, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(type(loss))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CheckpointFunctionBackward' object has no attribute 'input_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/function.py:301\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/adjoint.py:134\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.backward\u001b[0;34m(ctx, *grad_y)\u001b[0m\n\u001b[1;32m    131\u001b[0m     time_vjps[i] \u001b[38;5;241m=\u001b[39m dLd_cur_t\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Run the augmented system backwards in time.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m aug_state \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_dynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maug_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_rtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_atol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_options\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m aug_state \u001b[38;5;241m=\u001b[39m [a[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m aug_state]  \u001b[38;5;66;03m# extract just the t[i - 1] value\u001b[39;00m\n\u001b[1;32m    140\u001b[0m aug_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m y[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# update to use our forward-pass estimate of the state\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/odeint.py:79\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     76\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/solvers.py:114\u001b[0m, in \u001b[0;36mFixedGridODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    112\u001b[0m dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mcallback_step(t0, y0, dt)\n\u001b[0;32m--> 114\u001b[0m dy, f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m y1 \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m dy\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;129;01mand\u001b[39;00m t1 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m t[j]:\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/fixed_grid.py:28\u001b[0m, in \u001b[0;36mRK4._step_func\u001b[0;34m(self, func, t0, dt, t1, y0)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, t0, dt, t1, y0):\n\u001b[0;32m---> 28\u001b[0m     f0 \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEXT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPerturb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rk4_alt_step_func(func, t0, dt, t1, y0, f0\u001b[38;5;241m=\u001b[39mf0, perturb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperturb), f0\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:197\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:165\u001b[0m, in \u001b[0;36m_ReverseFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmul \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:144\u001b[0m, in \u001b[0;36m_TupleFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 144\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_flat_to_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([f_\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f_ \u001b[38;5;129;01min\u001b[39;00m f])\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torchdiffeq/_impl/adjoint.py:94\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.backward.<locals>.augmented_dynamics\u001b[0;34m(t, y_aug)\u001b[0m\n\u001b[1;32m     91\u001b[0m     _y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_strided(y, (), ())  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     _params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(torch\u001b[38;5;241m.\u001b[39mas_strided(param, (), ()) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m adjoint_params)  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     vjp_t, vjp_y, \u001b[38;5;241m*\u001b[39mvjp_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madjoint_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43madj_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# autograd.grad returns None if no gradient, set to zero.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m vjp_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(t) \u001b[38;5;28;01mif\u001b[39;00m vjp_t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m vjp_t\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    409\u001b[0m         grad_outputs_\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    425\u001b[0m     ):\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/autograd/function.py:301\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 137\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *output_grads)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, \u001b[38;5;241m*\u001b[39moutput_grads):\n\u001b[0;32m--> 137\u001b[0m     ctx\u001b[38;5;241m.\u001b[39minput_tensors \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tensors\u001b[49m]\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;66;03m# Fixes a bug where the first op in run_function modifies the\u001b[39;00m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m# Tensor storage in place, which is not allowed for detach()'d\u001b[39;00m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;66;03m# Tensors.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m         shallow_copies \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mview_as(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_tensors]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CheckpointFunctionBackward' object has no attribute 'input_tensors'"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
