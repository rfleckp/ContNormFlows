{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "#!pip install torchdiffeq\n",
    "#!pip install POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot as ot\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchdiffeq import odeint_adjoint as odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"From https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/optimal_transport.py\"\"\"\n",
    "import math\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import ot as pot\n",
    "import torch\n",
    "\n",
    "\n",
    "class OTPlanSampler:\n",
    "    \"\"\"OTPlanSampler implements sampling coordinates according to an OT plan (wrt squared Euclidean\n",
    "    cost) with different implementations of the plan calculation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: str,\n",
    "        reg: float = 0.05,\n",
    "        reg_m: float = 1.0,\n",
    "        normalize_cost: bool = False,\n",
    "        warn: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the OTPlanSampler class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method: str\n",
    "            choose which optimal transport solver you would like to use.\n",
    "            Currently supported are [\"exact\", \"sinkhorn\", \"unbalanced\",\n",
    "            \"partial\"] OT solvers.\n",
    "        reg: float, optional\n",
    "            regularization parameter to use for Sinkhorn-based iterative solvers.\n",
    "        reg_m: float, optional\n",
    "            regularization weight for unbalanced Sinkhorn-knopp solver.\n",
    "        normalize_cost: bool, optional\n",
    "            normalizes the cost matrix so that the maximum cost is 1. Helps\n",
    "            stabilize Sinkhorn-based solvers. Should not be used in the vast\n",
    "            majority of cases.\n",
    "        warn: bool, optional\n",
    "            if True, raises a warning if the algorithm does not converge\n",
    "        \"\"\"\n",
    "        # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n",
    "        # M is a cost matrix\n",
    "        if method == \"exact\":\n",
    "            self.ot_fn = pot.emd\n",
    "        elif method == \"sinkhorn\":\n",
    "            self.ot_fn = partial(pot.sinkhorn, reg=reg)\n",
    "        elif method == \"unbalanced\":\n",
    "            self.ot_fn = partial(pot.unbalanced.sinkhorn_knopp_unbalanced, reg=reg, reg_m=reg_m)\n",
    "        elif method == \"partial\":\n",
    "            self.ot_fn = partial(pot.partial.entropic_partial_wasserstein, reg=reg)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        self.reg = reg\n",
    "        self.reg_m = reg_m\n",
    "        self.normalize_cost = normalize_cost\n",
    "        self.warn = warn\n",
    "\n",
    "    def get_map(self, x0, x1):\n",
    "        \"\"\"Compute the OT plan (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : numpy array, shape (bs, bs)\n",
    "            represents the OT plan between minibatches\n",
    "        \"\"\"\n",
    "        a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n",
    "        if x0.dim() > 2:\n",
    "            x0 = x0.reshape(x0.shape[0], -1)\n",
    "        if x1.dim() > 2:\n",
    "            x1 = x1.reshape(x1.shape[0], -1)\n",
    "        x1 = x1.reshape(x1.shape[0], -1)\n",
    "        M = torch.cdist(x0, x1) ** 2\n",
    "        if self.normalize_cost:\n",
    "            M = M / M.max()  # should not be normalized when using minibatches\n",
    "        p = self.ot_fn(a, b, M.detach().cpu().numpy())\n",
    "        if not np.all(np.isfinite(p)):\n",
    "            print(\"ERROR: p is not finite\")\n",
    "            print(p)\n",
    "            print(\"Cost mean, max\", M.mean(), M.max())\n",
    "            print(x0, x1)\n",
    "        if np.abs(p.sum()) < 1e-8:\n",
    "            if self.warn:\n",
    "                warnings.warn(\"Numerical errors in OT plan, reverting to uniform plan.\")\n",
    "            p = np.ones_like(p) / p.size\n",
    "        return p\n",
    "\n",
    "    def sample_map(self, pi, batch_size, replace=True):\n",
    "        r\"\"\"Draw source and target samples from pi  $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pi : numpy array, shape (bs, bs)\n",
    "            represents the source minibatch\n",
    "        batch_size : int\n",
    "            represents the OT plan between minibatches\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (i_s, i_j) : tuple of numpy arrays, shape (bs, bs)\n",
    "            represents the indices of source and target data samples from $\\pi$\n",
    "        \"\"\"\n",
    "        p = pi.flatten()\n",
    "        p = p / p.sum()\n",
    "        choices = np.random.choice(\n",
    "            pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=replace\n",
    "        )\n",
    "        return np.divmod(choices, pi.shape[1])\n",
    "\n",
    "    def sample_plan(self, x0, x1, replace=True):\n",
    "        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch and draw source and target samples from pi $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        x1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        \"\"\"\n",
    "        pi = self.get_map(x0, x1)\n",
    "        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n",
    "        return x0[i], x1[j]\n",
    "\n",
    "    def sample_plan_with_labels(self, x0, x1, y0=None, y1=None, replace=True):\n",
    "        r\"\"\"Compute the OT plan $\\pi$ (wrt squared Euclidean cost) between a source and a target\n",
    "        minibatch and draw source and target labeled samples from pi $(x,z) \\sim \\pi$\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        y0 : Tensor, shape (bs)\n",
    "            represents the source label minibatch\n",
    "        y1 : Tensor, shape (bs)\n",
    "            represents the target label minibatch\n",
    "        replace : bool\n",
    "            represents sampling or without replacement from the OT plan\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch drawn from $\\pi$\n",
    "        x1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch drawn from $\\pi$\n",
    "        y0[i] : Tensor, shape (bs, *dim)\n",
    "            represents the source label minibatch drawn from $\\pi$\n",
    "        y1[j] : Tensor, shape (bs, *dim)\n",
    "            represents the target label minibatch drawn from $\\pi$\n",
    "        \"\"\"\n",
    "        pi = self.get_map(x0, x1)\n",
    "        i, j = self.sample_map(pi, x0.shape[0], replace=replace)\n",
    "        return (\n",
    "            x0[i],\n",
    "            x1[j],\n",
    "            y0[i] if y0 is not None else None,\n",
    "            y1[j] if y1 is not None else None,\n",
    "        )\n",
    "\n",
    "    def sample_trajectory(self, X):\n",
    "        \"\"\"Compute the OT trajectories between different sample populations moving from the source\n",
    "        to the target distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Tensor, (bs, times, *dim)\n",
    "            different populations of samples moving from the source to the target distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        to_return : Tensor, (bs, times, *dim)\n",
    "            represents the OT sampled trajectories over time.\n",
    "        \"\"\"\n",
    "        times = X.shape[1]\n",
    "        pis = []\n",
    "        for t in range(times - 1):\n",
    "            pis.append(self.get_map(X[:, t], X[:, t + 1]))\n",
    "\n",
    "        indices = [np.arange(X.shape[0])]\n",
    "        for pi in pis:\n",
    "            j = []\n",
    "            for i in indices[-1]:\n",
    "                j.append(np.random.choice(pi.shape[1], p=pi[i] / pi[i].sum()))\n",
    "            indices.append(np.array(j))\n",
    "\n",
    "        to_return = []\n",
    "        for t in range(times):\n",
    "            to_return.append(X[:, t][indices[t]])\n",
    "        to_return = np.stack(to_return, axis=1)\n",
    "        return to_return\n",
    "\n",
    "\n",
    "def wasserstein(\n",
    "    x0: torch.Tensor,\n",
    "    x1: torch.Tensor,\n",
    "    method: Optional[str] = None,\n",
    "    reg: float = 0.05,\n",
    "    power: int = 2,\n",
    "    **kwargs,\n",
    ") -> float:\n",
    "    \"\"\"Compute the Wasserstein (1 or 2) distance (wrt Euclidean cost) between a source and a target\n",
    "    distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x0 : Tensor, shape (bs, *dim)\n",
    "        represents the source minibatch\n",
    "    x1 : Tensor, shape (bs, *dim)\n",
    "        represents the source minibatch\n",
    "    method : str (default : None)\n",
    "        Use exact Wasserstein or an entropic regularization\n",
    "    reg : float (default : 0.05)\n",
    "        Entropic regularization coefficients\n",
    "    power : int (default : 2)\n",
    "        power of the Wasserstein distance (1 or 2)\n",
    "    Returns\n",
    "    -------\n",
    "    ret : float\n",
    "        Wasserstein distance\n",
    "    \"\"\"\n",
    "    assert power == 1 or power == 2\n",
    "    # ot_fn should take (a, b, M) as arguments where a, b are marginals and\n",
    "    # M is a cost matrix\n",
    "    if method == \"exact\" or method is None:\n",
    "        ot_fn = pot.emd2\n",
    "    elif method == \"sinkhorn\":\n",
    "        ot_fn = partial(pot.sinkhorn2, reg=reg)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    a, b = pot.unif(x0.shape[0]), pot.unif(x1.shape[0])\n",
    "    if x0.dim() > 2:\n",
    "        x0 = x0.reshape(x0.shape[0], -1)\n",
    "    if x1.dim() > 2:\n",
    "        x1 = x1.reshape(x1.shape[0], -1)\n",
    "    M = torch.cdist(x0, x1)\n",
    "    if power == 2:\n",
    "        M = M**2\n",
    "    ret = ot_fn(a, b, M.detach().cpu().numpy(), numItermax=int(1e7))\n",
    "    if power == 2:\n",
    "        ret = math.sqrt(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements Conditional Flow Matcher Losses. From https://github.com/atong01/conditional-flow-matching/blob/main/torchcfm/conditional_flow_matching.py\"\"\"\n",
    "\n",
    "# Author: Alex Tong\n",
    "#         Kilian Fatras\n",
    "#         +++\n",
    "# License: MIT License\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def pad_t_like_x(t, x):\n",
    "    \"\"\"Function to reshape the time vector t by the number of dimensions of x.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor, shape (bs, *dim)\n",
    "        represents the source minibatch\n",
    "    t : FloatTensor, shape (bs)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t : Tensor, shape (bs, number of x dimensions)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    x: Tensor (bs, C, W, H)\n",
    "    t: Vector (bs)\n",
    "    pad_t_like_x(t, x): Tensor (bs, 1, 1, 1)\n",
    "    \"\"\"\n",
    "    if isinstance(t, (float, int)):\n",
    "        return t\n",
    "    return t.reshape(-1, *([1] * (x.dim() - 1)))\n",
    "\n",
    "\n",
    "class ConditionalFlowMatcher:\n",
    "    \"\"\"Base class for conditional flow matching methods. This class implements the independent\n",
    "    conditional flow matching methods from [1] and serves as a parent class for all other flow\n",
    "    matching methods.\n",
    "\n",
    "    It implements:\n",
    "    - Drawing data from gaussian probability path N(t * x1 + (1 - t) * x0, sigma) function\n",
    "    - conditional flow matching ut(x1|x0) = x1 - x0\n",
    "    - score function $\\nabla log p_t(x|x0, x1)$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Union[float, int] = 0.0):\n",
    "        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma : Union[float, int]\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def compute_mu_t(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Compute the mean of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mean mu_t: t * x1 + (1 - t) * x0\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        t = pad_t_like_x(t, x0)\n",
    "        return t * x1 + (1 - t) * x0\n",
    "\n",
    "    def compute_sigma_t(self, t):\n",
    "        \"\"\"\n",
    "        Compute the standard deviation of the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        standard deviation sigma\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        del t\n",
    "        return self.sigma\n",
    "\n",
    "    def sample_xt(self, x0, x1, t, epsilon):\n",
    "        \"\"\"\n",
    "        Draw a sample from the probability path N(t * x1 + (1 - t) * x0, sigma), see (Eq.14) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "        epsilon : Tensor, shape (bs, *dim)\n",
    "            noise sample from N(0, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        mu_t = self.compute_mu_t(x0, x1, t)\n",
    "        sigma_t = self.compute_sigma_t(t)\n",
    "        sigma_t = pad_t_like_x(sigma_t, x0)\n",
    "        return mu_t + sigma_t * epsilon\n",
    "\n",
    "    def compute_conditional_flow(self, x0, x1, t, xt):\n",
    "        \"\"\"\n",
    "        Compute the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        del t, xt\n",
    "        return x1 - x0\n",
    "\n",
    "    def sample_noise_like(self, x):\n",
    "        return torch.randn_like(x)\n",
    "\n",
    "    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n",
    "        \"\"\"\n",
    "        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n",
    "        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        (optionally) t : Tensor, shape (bs)\n",
    "            represents the time levels\n",
    "            if None, drawn from uniform [0,1]\n",
    "        return_noise : bool\n",
    "            return the noise sample epsilon\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "        (optionally) eps: Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        if t is None:\n",
    "            t = torch.rand(x0.shape[0]).type_as(x0)\n",
    "        assert len(t) == x0.shape[0], \"t has to have batch size dimension\"\n",
    "\n",
    "        eps = self.sample_noise_like(x0)\n",
    "        xt = self.sample_xt(x0, x1, t, eps)\n",
    "        ut = self.compute_conditional_flow(x0, x1, t, xt)\n",
    "        if return_noise:\n",
    "            return t, xt, ut, eps\n",
    "        else:\n",
    "            return t, xt, ut\n",
    "\n",
    "    def compute_lambda(self, t):\n",
    "        \"\"\"Compute the lambda function, see Eq.(23) [3].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : FloatTensor, shape (bs)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lambda : score weighting function\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [4] Simulation-free Schrodinger bridges via score and flow matching, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        sigma_t = self.compute_sigma_t(t)\n",
    "        return 2 * sigma_t / (self.sigma**2 + 1e-8)\n",
    "\n",
    "\n",
    "class ExactOptimalTransportConditionalFlowMatcher(ConditionalFlowMatcher):\n",
    "    \"\"\"Child class for optimal transport conditional flow matching method. This class implements\n",
    "    the OT-CFM methods from [1] and inherits the ConditionalFlowMatcher parent class.\n",
    "\n",
    "    It overrides the sample_location_and_conditional_flow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Union[float, int] = 0.0):\n",
    "        r\"\"\"Initialize the ConditionalFlowMatcher class. It requires the hyper-parameter $\\sigma$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma : Union[float, int]\n",
    "        ot_sampler: exact OT method to draw couplings (x0, x1) (see Eq.(17) [1]).\n",
    "        \"\"\"\n",
    "        super().__init__(sigma)\n",
    "        self.ot_sampler = OTPlanSampler(method=\"exact\")\n",
    "\n",
    "    def sample_location_and_conditional_flow(self, x0, x1, t=None, return_noise=False):\n",
    "        r\"\"\"\n",
    "        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n",
    "        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n",
    "        with respect to the minibatch OT plan $\\Pi$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        (optionally) t : Tensor, shape (bs)\n",
    "            represents the time levels\n",
    "            if None, drawn from uniform [0,1]\n",
    "        return_noise : bool\n",
    "            return the noise sample epsilon\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        x0, x1 = self.ot_sampler.sample_plan(x0, x1)\n",
    "        return super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n",
    "\n",
    "    def guided_sample_location_and_conditional_flow(\n",
    "        self, x0, x1, y0=None, y1=None, t=None, return_noise=False\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Compute the sample xt (drawn from N(t * x1 + (1 - t) * x0, sigma))\n",
    "        and the conditional vector field ut(x1|x0) = x1 - x0, see Eq.(15) [1]\n",
    "        with respect to the minibatch OT plan $\\Pi$.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0 : Tensor, shape (bs, *dim)\n",
    "            represents the source minibatch\n",
    "        x1 : Tensor, shape (bs, *dim)\n",
    "            represents the target minibatch\n",
    "        y0 : Tensor, shape (bs) (default: None)\n",
    "            represents the source label minibatch\n",
    "        y1 : Tensor, shape (bs) (default: None)\n",
    "            represents the target label minibatch\n",
    "        (optionally) t : Tensor, shape (bs)\n",
    "            represents the time levels\n",
    "            if None, drawn from uniform [0,1]\n",
    "        return_noise : bool\n",
    "            return the noise sample epsilon\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t : FloatTensor, shape (bs)\n",
    "        xt : Tensor, shape (bs, *dim)\n",
    "            represents the samples drawn from probability path pt\n",
    "        ut : conditional vector field ut(x1|x0) = x1 - x0\n",
    "        (optionally) epsilon : Tensor, shape (bs, *dim) such that xt = mu_t + sigma_t * epsilon\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        [1] Improving and Generalizing Flow-Based Generative Models with minibatch optimal transport, Preprint, Tong et al.\n",
    "        \"\"\"\n",
    "        x0, x1, y0, y1 = self.ot_sampler.sample_plan_with_labels(x0, x1, y0, y1)\n",
    "        if return_noise:\n",
    "            t, xt, ut, eps = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n",
    "            return t, xt, ut, y0, y1, eps\n",
    "        else:\n",
    "            t, xt, ut = super().sample_location_and_conditional_flow(x0, x1, t, return_noise)\n",
    "            return t, xt, ut, y0, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"modified from https://github.com/rtqichen/torchdiffeq/blob/master/examples/odenet_mnist.py\"\"\"\n",
    "\n",
    "class ConcatConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=1, dilation=1, groups=1, bias=True, transpose=False):\n",
    "        super(ConcatConv2d, self).__init__()\n",
    "        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n",
    "        self._layer = module(\n",
    "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if t.dim() != 0:\n",
    "            t = t.view(x.size(0), 1, 1, 1)\n",
    "        tt = torch.ones_like(x[:, :1, :, :]) * t\n",
    "        ttx = torch.cat([tt, x], 1)\n",
    "        return self._layer(ttx)\n",
    "    \n",
    "\n",
    "\"\"\"Stolen from https://github.com/cfinlay/ffjord-rnode/blob/master/lib/layers/odefunc.py\"\"\"\n",
    "\n",
    "def divergence_approx(f, y, e=None):\n",
    "\n",
    "    samples = []\n",
    "    sqnorms = []\n",
    "    for  e_ in e:\n",
    "        e_dzdx = torch.autograd.grad(f, y, e_, create_graph=True)[0]\n",
    "        n = e_dzdx.view(y.size(0),-1).pow(2).mean(dim=1, keepdim=True)\n",
    "        sqnorms.append(n)\n",
    "        e_dzdx_e = e_dzdx * e_\n",
    "        samples.append(e_dzdx_e.view(y.shape[0], -1).sum(dim=1, keepdim=True))\n",
    "\n",
    "    S = torch.cat(samples, dim=1)\n",
    "    approx_tr_dzdx = S.mean(dim=1)\n",
    "\n",
    "    N = torch.cat(sqnorms, dim=1).mean(dim=1)\n",
    "\n",
    "\n",
    "    return approx_tr_dzdx, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Augmented ODE to train RNODE\"\"\"\n",
    "class regularized_Augmented_ODE2(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "\n",
    "        z = states[0]         \n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)                                          #dynamics \n",
    "            dz_dt = self.odefunc(t, z)\n",
    "            epsilon = torch.randn_like(z)\n",
    "            dlog_det_dt, dn_dt = divergence_approx(dz_dt, z, e=epsilon.unsqueeze(0))\n",
    "            dE_dt = (torch.linalg.matrix_norm(dz_dt)**2)          #kinetic Energy\n",
    "\n",
    "            \"\"\"            \n",
    "            print(\"dz_dt: \", dz_dt.shape)\n",
    "            print(\"dlog_det_dt: \", dlog_det_dt.shape)\n",
    "            print(\"dE_dt: \", dE_dt.shape)\n",
    "            print(\"dn_dt: \", dn_dt.shape)\n",
    "            \"\"\"\n",
    "            return (dz_dt, dlog_det_dt, dE_dt, dn_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = ConcatConv2d(dim_in=1, dim_out=64, stride=1)\n",
    "        self.conv2 = ConcatConv2d(dim_in=64, dim_out=64, stride=2)\n",
    "        self.conv3 = ConcatConv2d(dim_in=64, dim_out=128, stride=1)\n",
    "        self.conv4 = ConcatConv2d(dim_in=128, dim_out=128, stride=2)\n",
    "\n",
    "        self.Tconv1 = ConcatConv2d(dim_in=128, dim_out=128, stride=1, transpose=True)\n",
    "        self.Tconv2 = ConcatConv2d(dim_in=128, dim_out=64, ksize=4, stride=2, transpose=True)\n",
    "        self.Tconv3 = ConcatConv2d(dim_in=64, dim_out=64, stride=1, transpose=True)\n",
    "        self.Tconv4 = ConcatConv2d(dim_in=64, dim_out=1, ksize=4, stride=2, transpose=True)\n",
    "        self.activ = nn.Softplus()\n",
    "    \n",
    "    def forward(self, t, x):\n",
    "        out = self.conv1(t, x)\n",
    "        out = self.activ(out)\n",
    "        out = self.conv2(t, out)\n",
    "        out = self.activ(out)\n",
    "        out = self.conv3(t, out)\n",
    "        out = self.activ(out)\n",
    "        out = self.conv4(t, out)\n",
    "        out = self.activ(out)\n",
    "\n",
    "        out = self.Tconv1(t, out)\n",
    "        out = self.activ(out)\n",
    "        out = self.Tconv2(t, out)\n",
    "        out = self.activ(out)\n",
    "        out = self.Tconv3(t, out)\n",
    "        out = self.activ(out)\n",
    "        out = self.Tconv4(t, out)\n",
    "        out = self.activ(out)\n",
    "\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dynamics, i.e., right hand side of the ODE\"\"\"\n",
    "class NODE(nn.Module):\n",
    "    def __init__(self, convnet):\n",
    "        super().__init__()\n",
    "        self.net = convnet\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(t, x)\n",
    "    \n",
    "    \"\"\"plots the ode solutions from its initial value x at time 0 to the final solution at time 1 (t decides number of steps)\"\"\"\n",
    "    def plot_trajectories(self, trajectories=None, t=None, x=None):\n",
    "        fig, ax = plt.subplots()\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "\n",
    "            ax.plot(trajectories[:,:,0], trajectories[:,:,1], color=\"silver\", alpha=.7, zorder=0, label=\"flow\")            \n",
    "            ax.scatter(trajectories[0,:,0],trajectories[0,:,1], c=\"royalblue\", s=1, label=\"prior sample\", zorder=1)\n",
    "            ax.scatter(trajectories[-1,:,0], trajectories[-1,:,1], c=\"darkorange\", s=1, label=\"gen sample\", zorder=1)\n",
    "\n",
    "            handles, labels = plt.gca().get_legend_handles_labels()\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            plt.legend(by_label.values(), by_label.keys())\n",
    "        \n",
    "        return fig, ax\n",
    "    \n",
    "    \"\"\"log probability of the model with a normal gaussian as the initial distribution\"\"\"\n",
    "    def log_probability(self, t, x):\n",
    "        initial_distr = torch.distributions.MultivariateNormal(torch.zeros(28), torch.eye(28))\n",
    "\n",
    "        l0 = torch.zeros((x.size(0),1), requires_grad=False)\n",
    "        initial_values = (x, l0)\n",
    "\n",
    "        augmented_dynamics = trace_Augmented_ODE(self)\n",
    "\n",
    "        z_t, log_det = odeint(augmented_dynamics, initial_values, t, method=\"rk4\")\n",
    "\n",
    "        logp_x = initial_distr.log_prob(z_t[-1]) + log_det[-1]\n",
    "\n",
    "        return -logp_x.mean()\n",
    "    \n",
    "    \"\"\"computes the average length of the trajectories from the initial values x to the values at the final timestep\"\"\"\n",
    "    def length_trajectories(self, trajectories=None, t=None, x=None):\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "            distances = torch.linalg.norm(trajectories[1:] - trajectories[:-1], dim=-1)\n",
    "            length = distances.sum(dim=0)\n",
    "\n",
    "            return length.mean()\n",
    "        \n",
    "    \"computes the Wasserstein2 distance between samples generated from the initial values x and samples y from the true distribution\"\n",
    "    def wasserstein2_distance(self, y, trajectories=None, t=None, x=None):\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "            z = trajectories[-1]\n",
    "            a, b = ot.unif(z.size(0)), ot.unif(y.size(0))\n",
    "            cost = torch.cdist(z, y) ** 2\n",
    "            distance = ot.emd2(a, b, cost.numpy())\n",
    "\n",
    "            return distance**.5\n",
    "\n",
    "    \n",
    "\"\"\"Augmented ODE for CNF without any regularization and choice of samples for the Hutchinson trace estimator and no gradient\"\"\"\n",
    "class trace_Augmented_ODE(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        with torch.no_grad():\n",
    "            z = states[0]\n",
    "            dz_dt, dlogp_z_dt =  self.hutchinson_trace_estimator(t, z)\n",
    "            return (dz_dt, dlogp_z_dt)\n",
    "        \n",
    "    def hutchinson_trace_estimator(self, t, z, samples=20):\n",
    "        trace = 0\n",
    "        \n",
    "        for _ in range(samples):\n",
    "            epsilon = torch.randn_like(z)\n",
    "            output_f, vjp_f = torch.autograd.functional.vjp(self.odefunc, (t,z), v=epsilon, create_graph=False)\n",
    "            trace +=  (vjp_f[1]*epsilon).sum(1).unsqueeze(1)    \n",
    "\n",
    "        return output_f, trace/samples\n",
    "    \n",
    "\"\"\"Augmented ODE to train NODE\"\"\"\n",
    "class Augmented_ODE(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z = states[0]                                                               #dynamics \n",
    "            dz_dt, vjp_f, epsilon = self.vjp(self.odefunc, t, z)\n",
    "            dlog_det_dt =  (vjp_f*epsilon).sum(-1).sum(-1)                     #log-det of the Jacobian   \n",
    "            \n",
    "            return (dz_dt, dlog_det_dt)\n",
    "\n",
    "    def vjp(self, f, t, z):\n",
    "        \"\"\"computes vector Jacobian product and returns (output of the function, vjp, epsilon)\"\"\"\n",
    "        epsilon = torch.randn_like(z)\n",
    "        output_f, vjp_f = torch.autograd.functional.vjp(f, (t,z), v=epsilon, create_graph=True)\n",
    "        return output_f, vjp_f[1], epsilon\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Augmented ODE to train RNODE\"\"\"\n",
    "class regularized_Augmented_ODE(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z = states[0]                                                               #dynamics \n",
    "            dz_dt, vjp_f, epsilon = self.vjp(self.odefunc, t, z)\n",
    "            dlog_det_dt =  (vjp_f*epsilon).sum(-1).sum(-1)                     #log-det of the Jacobian   \n",
    "            dE_dt = (torch.linalg.matrix_norm(dz_dt)**2)          #kinetic Energy\n",
    "            dn_dt = (torch.linalg.matrix_norm(vjp_f)**2)          #Frobenius norm of the Jacobian\n",
    "\n",
    "            \"\"\"            \n",
    "            print(\"dz_dt: \", dz_dt.shape)\n",
    "            print(\"dlog_det_dt: \", dlog_det_dt.shape)\n",
    "            print(\"dE_dt: \", dE_dt.shape)\n",
    "            print(\"dn_dt: \", dn_dt.shape)\n",
    "            \"\"\"\n",
    "            return (dz_dt, dlog_det_dt, dE_dt, dn_dt)\n",
    "\n",
    "    def vjp(self, f, t, z):\n",
    "        \"\"\"computes vector Jacobian product and returns (output of the function, vjp, epsilon)\"\"\"\n",
    "        epsilon = torch.randn_like(z)\n",
    "        output_f, vjp_f = torch.autograd.functional.vjp(f, (t,z), v=epsilon, create_graph=True)\n",
    "        return output_f, vjp_f[1], epsilon\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\"computes the loss of RNODE, given the dynamics 'model' and samples 'x'\"\"\"\n",
    "def rnode_loss(model: torch.nn.Module, x, lambda_k: float, lambda_j: float):\n",
    "    initial_distr = torch.distributions.MultivariateNormal(torch.zeros(28), torch.eye(28))\n",
    "    t = torch.linspace(0, 1, 5).type(torch.float32)\n",
    "    l0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    kin_E0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    n0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    initial_values = (x, l0, kin_E0, n0)\n",
    "\n",
    "    augmented_dynamics = regularized_Augmented_ODE(model)\n",
    "\n",
    "    z_t, log_det, E_t, n_t = odeint(augmented_dynamics, initial_values, t,\n",
    "                                    method=\"rk4\")\n",
    "\n",
    "    z1, l1, kin_E1, n1 = z_t[-1], log_det[-1], E_t[-1], n_t[-1]\n",
    "\n",
    "    logp_x = initial_distr.log_prob(z1) + l1 - lambda_k * kin_E1 - lambda_j * n1\n",
    "    loss = -logp_x.mean()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnode_training(epochs, seed, learning_rate, lambda_k, lambda_j):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model = CNN()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    Path(\"mnist/rnode/models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "        num = 0\n",
    "        for i, (samples, labels) in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            #samples.requires_grad = True\n",
    "\n",
    "            x0 = samples\n",
    "            initial_distr = torch.distributions.MultivariateNormal(torch.zeros(28), torch.eye(28))\n",
    "            t = torch.linspace(0, 1, 5).type(torch.float32)\n",
    "            l0 = torch.zeros((x0.size(0),1), requires_grad=True)\n",
    "            kin_E0 = torch.zeros((x0.size(0),1), requires_grad=True)\n",
    "            n0 = torch.zeros((x0.size(0),1), requires_grad=True)\n",
    "            initial_values = (x0, l0, kin_E0, n0)\n",
    "\n",
    "            augmented_dynamics = regularized_Augmented_ODE(model)\n",
    "\n",
    "            z_t, log_det, E_t, n_t = odeint(augmented_dynamics, initial_values, t, method=\"rk4\")\n",
    "            z1, l1, kin_E1, n1 = z_t[-1], log_det[-1], E_t[-1], n_t[-1]\n",
    "\n",
    "            logp_x = initial_distr.log_prob(z1) + l1 - lambda_k * kin_E1 - lambda_j * n1\n",
    "            loss = -logp_x.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            num += 1\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(\"mnist/rnode/models\", f\"{epoch}_model.pt\"))\n",
    "        print(f\"\\nloss: {loss/num}\")\n",
    "        \n",
    "\n",
    "    print(\"finished training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training loop for OT-CFM\"\"\"\n",
    "def cfm_training(epochs, seed, learning_rate):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    sigma = 0.0\n",
    "    model = CNN()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    FM = ExactOptimalTransportConditionalFlowMatcher(sigma=sigma)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    start = time.time()\n",
    "    Path(\"mnist/cfm/models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "        num = 0\n",
    "        for i, (samples, labels) in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x0 = torch.randn_like(samples)\n",
    "            t, xt, ut = FM.sample_location_and_conditional_flow(x0, samples)\n",
    "            vt = model(t, xt)\n",
    "            loss = torch.mean((vt - ut) ** 2)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss\n",
    "            num += 1\n",
    "        \n",
    "        torch.save(model.state_dict(), os.path.join(\"mnist/cfm/models\", f\"{epoch}_model.pt\"))\n",
    "        print(f\"\\nloss: {epoch_loss/num}\")\n",
    "        \n",
    "\n",
    "    print(\"finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hyperparameters_and_metrics(file_path: str, hyperparameters: dict, metrics: dict):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(\"Hyperparameters:\\n\")\n",
    "        for param, value in hyperparameters.items():\n",
    "            file.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "        file.write(\"\\nMetrics per  batches:\\n\")\n",
    "        file.write(\"batches, loss, negative_log_likelihood, flow_length, wasserstein2_distance, elapsed_time\\n\")\n",
    "        for  batches in range(len(metrics[\"batches\"])):\n",
    "            file.write(f\"{metrics['batches'][batches]}, {metrics['loss'][batches]}, {metrics['log_probability'][batches]}, {metrics['flow_length'][batches]}, {metrics['wasserstein2_distance'][batches]}, {metrics['elapsed_time'][batches]}\\n\")\n",
    "        \n",
    "        file.write(f\"total time: {sum(metrics['elapsed_time'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 128\n",
    "n_epochs = 3\n",
    "learning_rate=1e-3\n",
    "seed = 44\n",
    "epochs = 3\n",
    "lambda_k = .1\n",
    "lambda_j = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   4%|▎         | 35/938 [00:07<03:21,  4.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcfm_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[235], line 27\u001b[0m, in \u001b[0;36mcfm_training\u001b[0;34m(epochs, seed, learning_rate)\u001b[0m\n\u001b[1;32m     25\u001b[0m x0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(samples)\n\u001b[1;32m     26\u001b[0m t, xt, ut \u001b[38;5;241m=\u001b[39m FM\u001b[38;5;241m.\u001b[39msample_location_and_conditional_flow(x0, samples)\n\u001b[0;32m---> 27\u001b[0m vt \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((vt \u001b[38;5;241m-\u001b[39m ut) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[231], line 19\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactiv(out)\n\u001b[1;32m     18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(t, out)\n\u001b[0;32m---> 19\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(t, out)\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactiv(out)\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Jupyter/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:859\u001b[0m, in \u001b[0;36mSoftplus.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftplus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfm_training(n_epochs, seed, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnode_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
