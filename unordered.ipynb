{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "#!pip install torchdyn\n",
    "#!pip install torchdiffeq\n",
    "#!pip install POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ot as ot\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "from torchdyn.datasets import generate_gaussians, generate_moons, generate_concentric_spheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Sampler and utils\"\"\"\n",
    "\n",
    "def sample_normal(n, mu=torch.zeros(2), sigma=1):\n",
    "    distr = torch.distributions.MultivariateNormal(mu, sigma*torch.eye(2))\n",
    "    return distr.sample((n,))\n",
    "\n",
    "def sample_gaussians(n, n_gaussians=7):\n",
    "    num = int(n/n_gaussians)+1\n",
    "    x0, _ = generate_gaussians(num, n_gaussians, radius=4, std_gaussians=.5)\n",
    "    return x0[:n]\n",
    "\n",
    "def sample_moons(n):\n",
    "    x0, _ = generate_moons(n, noise=0.2)\n",
    "    return x0 * 4 - 2\n",
    "\n",
    "def sample_circles(n):\n",
    "    x0, _ = generate_concentric_spheres(n , dim=2, inner_radius=.5, outer_radius=1)\n",
    "    return x0\n",
    "\n",
    "class sampler():\n",
    "    def __init__(self, dataset: str):\n",
    "        if dataset == \"normal\":\n",
    "            self.sampler = sample_normal\n",
    "        elif dataset == \"gaussians\":\n",
    "            self.sampler = sample_gaussians\n",
    "        elif dataset == \"moons\":\n",
    "            self.sampler = sample_moons\n",
    "        elif dataset == \"circles\":\n",
    "            self.sampler = sample_circles\n",
    "        else:\n",
    "            raise Exception(\"Selected Dataset not supported. Choose between normal, gaussians, moons or circles\")\n",
    "\n",
    "    def __call__(self, n: int):\n",
    "        return self.sampler(n)\n",
    "\n",
    "\"\"\"utils\"\"\"\n",
    "def initial_setup(x, y):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x[:,0],x[:,1], c=\"royalblue\", s=1, label=\"initial dataset\")\n",
    "    ax.scatter(y[:,0], y[:,1], c=\"darkorange\", s=1, label=\"target dataset\")\n",
    "    ax.set_title(\"initial and target distributions\")\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def save_hyperparameters_and_metrics(file_path: str, hyperparameters: dict, metrics: dict):\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(\"Hyperparameters:\\n\")\n",
    "        for param, value in hyperparameters.items():\n",
    "            file.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "        file.write(\"\\nMetrics per  batches:\\n\")\n",
    "        file.write(\"batches, loss, negative_log_likelihood, flow_length, wasserstein2_distance\\n\")\n",
    "        for  batches in range(len(metrics[\"batches\"])):\n",
    "            file.write(f\"{metrics[\"batches\"][batches]}, {metrics[\"loss\"][batches]}, {metrics[\"log_probability\"][batches]}, {metrics[\"flow_length\"][batches]}, {metrics[\"wasserstein2_distance\"][batches]}\\n\")\n",
    "\n",
    "def test_model(model_path: str, seed=44, samples=5):\n",
    "    a = model_path.split(\"/\")\n",
    "    b = a[3]\n",
    "    a = (a[1]).split(\"_\")\n",
    "    a.remove(\"to\")\n",
    "\n",
    "    model = MLP()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    if a[0]==\"cfm\":\n",
    "        str_model_type, initial, target = a\n",
    "        kin, frob = None, None\n",
    "\n",
    "    else:\n",
    "        str_model_type, target, kin, frob = a\n",
    "        initial = \"normal\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    initial_samples = sampler(initial)\n",
    "    target_samples = sampler(target)\n",
    "\n",
    "\n",
    "\n",
    "    log_probability = 0\n",
    "    flow_length = 0\n",
    "    wasserstein2_distance = 0\n",
    "\n",
    "    for _ in range(samples):\n",
    "        X = initial_samples(1000)\n",
    "        Y = target_samples(1000)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            trajectories = odeint(model, X, torch.linspace(1, 0, 100), method=\"rk4\")\n",
    "            log_probability += model.log_probability(torch.linspace(0, 1, 5), Y).item()\n",
    "            flow_length += model.length_trajectories(trajectories).item()\n",
    "            wasserstein2_distance += model.wasserstein2_distance(Y, trajectories)\n",
    "\n",
    "    return log_probability/samples, flow_length/samples, wasserstein2_distance/samples, b\n",
    "\n",
    "\"\"\"takes the path to a directory containing models as input and performs tests on all models in the directory. results are saved in test_log\"\"\"\n",
    "def test_models(models_dir: str):\n",
    "    models = [f for f in os.listdir(models_dir) if os.path.isfile(os.path.join(models_dir, f))]\n",
    "    log_path = \"/\".join(models_dir.split(\"/\")[:2]) + \"/training_log.txt\"\n",
    "    a = 0\n",
    "    with open(log_path, 'a') as test_file:\n",
    "        test_file.write(f\"\\ntests \\n\")\n",
    "        test_file.write(\"negative log-likelihood, flow length, wasserstein2 distance \\n\")\n",
    "        for model in models:\n",
    "            model_path = os.path.join(models_dir, model)\n",
    "            log_likelihood, flow_length, wasserstein2, batch = test_model(model_path)\n",
    "            test_file.write(batch + f\": {log_likelihood}, {flow_length}, {wasserstein2} \\n\")\n",
    "\n",
    "\n",
    "\"\"\"saves the model and the plots of the trajectories of the generated samples and updates the metrics at the current batch\"\"\"\n",
    "def save(model: torch.nn.Module, trajectories: torch.Tensor, metrics: dict, X, Y, loss, batch, args):\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(args.parameter_path + \"/models\", \"batch_\"+f\"{batch}_model.pt\"))         #saves the model\n",
    "\n",
    "    fig, ax = model.plot_trajectories(trajectories)\n",
    "    ax.set_title(f\"{args.model_type}_{args.initial_dataset}_to_\" + f\"{args.target_dataset}\")\n",
    "\n",
    "    fig.savefig(os.path.join(args.parameter_path + \"/trajectories\", \"batch_\"+f\"{batch}_trajectory.png\"))                #saves the plot of the trajectories\n",
    "    \n",
    "    metrics[\"batches\"].append(batch)\n",
    "    metrics[\"log_probability\"].append(model.log_probability(torch.linspace(0, 1, 5).type(torch.float32), Y))\n",
    "    metrics[\"loss\"].append(loss)\n",
    "    metrics[\"flow_length\"].append(model.length_trajectories(trajectories))\n",
    "    metrics[\"wasserstein2_distance\"].append(model.wasserstein2_distance(Y, trajectories))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\"\"\"Dynamics, i.e., right hand side of the ODE\"\"\"\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=None, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        if out_dim is None:\n",
    "            out_dim = in_dim\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim + 1, hidden_dim),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.SELU(),\n",
    "            torch.nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if t.dim()==0:\n",
    "            t = t.expand(x.size(0),)\n",
    "        x = torch.cat((x, t[:,None]),dim=-1) \n",
    "        return self.net(x)\n",
    "    \n",
    "    \"\"\"plots the ode solutions from its initial value x at time 0 to the final solution at time 1 (t decides number of steps)\"\"\"\n",
    "    def plot_trajectories(self, trajectories=None, t=None, x=None):\n",
    "        fig, ax = plt.subplots()\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "\n",
    "            ax.plot(trajectories[:,:,0], trajectories[:,:,1], color=\"silver\", alpha=.7, zorder=0, label=\"flow\")            \n",
    "            ax.scatter(trajectories[0,:,0],trajectories[0,:,1], c=\"royalblue\", s=1, label=\"prior sample\", zorder=1)\n",
    "            ax.scatter(trajectories[-1,:,0], trajectories[-1,:,1], c=\"darkorange\", s=1, label=\"gen sample\", zorder=1)\n",
    "\n",
    "            handles, labels = plt.gca().get_legend_handles_labels()\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            plt.legend(by_label.values(), by_label.keys())\n",
    "        \n",
    "        return fig, ax\n",
    "    \n",
    "    \"\"\"log probability of the model with a normal gaussian as the initial distribution, esto esta mal luego pregunta\"\"\"\n",
    "    def log_probability(self, t, x):\n",
    "        initial_distr = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "        l0 = torch.zeros((x.size(0),1), requires_grad=False)\n",
    "        initial_values = (x, l0)\n",
    "\n",
    "        augmented_dynamics = Augmented_ODE(self)\n",
    "\n",
    "        z_t, log_det = odeint(augmented_dynamics, initial_values, t, method=\"rk4\")\n",
    "\n",
    "        logp_x = initial_distr.log_prob(z_t[-1]) + log_det[-1]\n",
    "\n",
    "        return -logp_x.mean()\n",
    "    \n",
    "    \"\"\"computes the average length of the trajectories from the initial values x to the values at the final timestep\"\"\"\n",
    "    def length_trajectories(self, trajectories=None, t=None, x=None):\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "            distances = torch.linalg.norm(trajectories[1:] - trajectories[:-1], dim=-1)\n",
    "            length = distances.sum(dim=0)\n",
    "\n",
    "            return length.mean()\n",
    "        \n",
    "    \"computes the Wasserstein2 distance between samples generated from the initial values x and samples y from the true distribution\"\n",
    "    def wasserstein2_distance(self, y, trajectories=None, t=None, x=None):\n",
    "        with torch.no_grad():\n",
    "            if trajectories == None:\n",
    "                trajectories = odeint(self, x, t, method=\"rk4\")\n",
    "            z = trajectories[-1]\n",
    "            a, b = ot.unif(z.size(0)), ot.unif(y.size(0))\n",
    "            cost = torch.cdist(z, y) ** 2\n",
    "            distance = ot.emd2(a, b, cost.numpy())\n",
    "\n",
    "            return distance**.5\n",
    "\n",
    "    \n",
    "\"\"\"Augmented ODE for CNF without any regularization and choice of samples for the Hutchinson trace estimator\"\"\"\n",
    "class Augmented_ODE(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        with torch.no_grad():\n",
    "            z = states[0]\n",
    "            dz_dt, dlogp_z_dt =  self.hutchinson_trace_estimator(t, z)\n",
    "            return (dz_dt, dlogp_z_dt)\n",
    "        \n",
    "    def hutchinson_trace_estimator(self, t, z, samples=20):\n",
    "        trace = 0\n",
    "        \n",
    "        for _ in range(samples):\n",
    "            epsilon = torch.randn_like(z)\n",
    "            output_f, vjp_f = torch.autograd.functional.vjp(self.odefunc, (t,z), v=epsilon, create_graph=True)\n",
    "            trace +=  (vjp_f[1]*epsilon).sum(1).unsqueeze(1)    \n",
    "\n",
    "        return output_f, trace/samples\n",
    "\n",
    "\"\"\"Augmented ODE to train RNODE\"\"\"\n",
    "class regularized_Augmented_ODE(nn.Module):\n",
    "    def __init__(self, ode_func: nn.Module):\n",
    "        super().__init__()\n",
    "        self.odefunc = ode_func\n",
    "\n",
    "    def forward(self, t, states):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z = states[0]                                                               #dynamics \n",
    "            dz_dt, vjp_f, epsilon = self.vjp(self.odefunc, t, z)\n",
    "            dlog_det_dt =  (vjp_f*epsilon).sum(1).unsqueeze(1)                           #log-det of the Jacobian   \n",
    "            dE_dt = (torch.linalg.vector_norm(dz_dt, dim=1, keepdims=True)**2)          #kinetic Energy\n",
    "            dn_dt = (torch.linalg.vector_norm(vjp_f, dim=1, keepdims=True)**2)          #Frobenius norm of the Jacobian\n",
    "            return (dz_dt, dlog_det_dt, dE_dt, dn_dt)\n",
    "\n",
    "    def vjp(self, f, t, z):\n",
    "        \"\"\"computes vector Jacobian product and returns (output of the function, vjp, epsilon)\"\"\"\n",
    "        epsilon = torch.randn_like(z)\n",
    "        output_f, vjp_f = torch.autograd.functional.vjp(f, (t,z), v=epsilon, create_graph=True)\n",
    "        return output_f, vjp_f[1], epsilon\n",
    "\n",
    "\n",
    "\"\"\"Helper class to sample and compute the OT plan, as well as the conditional location and flow\"\"\"\n",
    "def padding(t, x):\n",
    "    if isinstance(t, (float, int)):\n",
    "        return t\n",
    "    return t.view([x.size(0)] + [1] * (x.dim() - 1))\n",
    "\n",
    "class Conditional_FM():\n",
    "    def __init__(self, sigma=0):\n",
    "        self.ot_solver = ot.emd\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def get_map(self, x0, x1):\n",
    "        a, b = ot.unif(x0.size(0)), ot.unif(x1.size(0))\n",
    "        cost = torch.cdist(x0, x1) ** 2\n",
    "        pi = self.ot_solver(a, b, cost.numpy())\n",
    "\n",
    "        return pi\n",
    "    \n",
    "    def sample_OT(self, x0, x1, batch_size: int):\n",
    "        pi = self.get_map(x0, x1)\n",
    "        p = pi.flatten()\n",
    "        p = p / p.sum()\n",
    "        choices = np.random.choice(\n",
    "            pi.shape[0] * pi.shape[1], p=p, size=batch_size)\n",
    "        i, j = np.divmod(choices, pi.shape[1])\n",
    "    \n",
    "        return x0[i], x1[j]  \n",
    "    \n",
    "    def sample_location_and_conditional_flow(self, x0, x1):\n",
    "        t = torch.rand(x0.shape[0]).type_as(x0)\n",
    "        var = torch.randn_like(x0)\n",
    "        padded_t = padding(t, x0)\n",
    "        sigma_t = padding(self.sigma, x0)\n",
    "        xt = padded_t * x1 + (1 - padded_t) * x0 +  sigma_t * var\n",
    "        ut = x1 - x0\n",
    "\n",
    "        return t, xt, ut  \n",
    "    \n",
    "    \n",
    "\"\"\"computes the loss of RNODE, given the dynamics 'model' and samples 'x'\"\"\"\n",
    "def rnode_loss(model: torch.nn.Module, x, args):\n",
    "    initial_distr = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "    t = torch.linspace(0, 1, 5).type(torch.float32)\n",
    "    l0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    kin_E0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    n0 = torch.zeros((x.size(0),1), requires_grad=True)\n",
    "    initial_values = (x, l0, kin_E0, n0)\n",
    "\n",
    "    augmented_dynamics = regularized_Augmented_ODE(model)\n",
    "\n",
    "    z_t, log_det, E_t, n_t = odeint(augmented_dynamics, initial_values, t,\n",
    "                                    method=args.odeint_method,\n",
    "                                    atol=args.odeint_atol,\n",
    "                                    rtol=args.odeint_rtol)\n",
    "\n",
    "    z1, l1, kin_E1, n1 = z_t[-1], log_det[-1], E_t[-1], n_t[-1]\n",
    "\n",
    "    logp_x = initial_distr.log_prob(z1) + l1 -args.reg_kinetic_energy * kin_E1 - args.reg_frobenius_norm * n1\n",
    "    loss = -logp_x.mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\"\"\"computes the loss of CFM given the dynamics 'model', initial and target samples 'x0' and 'x1' respectively and batch size 'bs'\"\"\"\n",
    "def cfm_loss(model: torch.nn.Module, x0, x1, bs):           #change order of x0 and x1 to obtain target at time 0 and initial at time 1\n",
    "    flow_matcher = Conditional_FM()\n",
    "    x0, x1 = flow_matcher.sample_OT(x0, x1, bs) \n",
    "    t, xt, ut = flow_matcher.sample_location_and_conditional_flow(x0, x1)\n",
    "    \n",
    "    loss = torch.mean((model(t,xt) - ut)**2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\"\"\"Training loop for RNODE\"\"\"\n",
    "def rnode_training(args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    model = MLP()\n",
    "\n",
    "    initial_samples = sampler(\"normal\")\n",
    "    target_samples = sampler(args.target_dataset)\n",
    "\n",
    "    X = initial_samples(1000)\n",
    "    Y = target_samples(1000)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    model.train()\n",
    "    batch_save = int(args.num_batches/5)\n",
    "    batch_loss = 0\n",
    "\n",
    "    metrics = {\n",
    "        \"batches\": [],\n",
    "        \"loss\": [],\n",
    "        \"log_probability\": [],\n",
    "        \"flow_length\": [],\n",
    "        \"wasserstein2_distance\": [],\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(1, args.num_batches+1), desc=\"Training Batches\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = target_samples(args.batch_size)\n",
    "        x.requires_grad = True\n",
    "\n",
    "        loss = rnode_loss(model, x, args)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss\n",
    "\n",
    "        if i == 1:\n",
    "            fig, ax = initial_setup(X, Y)\n",
    "            fig.savefig(os.path.join(args.parameter_path + \"/trajectories\", \"initial_target_distr.png\"))\n",
    "\n",
    "        elif i%batch_save == 0:\n",
    "            loss = batch_loss/batch_save\n",
    "            batch_loss = 0\n",
    "            print(f\"batch {i}, Loss: {loss}\")\n",
    "\n",
    "            trajectories = odeint(model, X, torch.linspace(1, 0, 100).type(torch.float32), method=\"rk4\")\n",
    "            metrics = save(model, trajectories, metrics, X, Y, loss, i, args)\n",
    "\n",
    "\n",
    "    print(\"finished training\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\"\"\"Training loop for OT-CFM\"\"\"\n",
    "def cfm_training(args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    model = MLP()\n",
    "\n",
    "    initial_samples = sampler(args.initial_dataset)\n",
    "    target_samples = sampler(args.target_dataset)\n",
    "\n",
    "    X = initial_samples(1000)\n",
    "    Y = target_samples(1000)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    model.train()\n",
    "    bs = args.batch_size\n",
    "    batch_save = int(args.num_batches/5)\n",
    "    batch_loss = 0\n",
    "\n",
    "    metrics = {\n",
    "        \"batches\": [],\n",
    "        \"loss\": [],\n",
    "        \"log_probability\": [],\n",
    "        \"flow_length\": [],\n",
    "        \"wasserstein2_distance\": []\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(1, args.num_batches+1), desc=\"Training Batches\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x0, x1 = initial_samples(bs), target_samples(bs)\n",
    "\n",
    "        loss = cfm_loss(model, x1, x0, bs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss\n",
    "\n",
    "        if i == 1:\n",
    "            fig, ax = initial_setup(X, Y)\n",
    "            fig.savefig(os.path.join(args.parameter_path + \"/trajectories\", \"initial_target_distr.png\"))\n",
    "\n",
    "        elif i%batch_save == 0:\n",
    "            loss = batch_loss/batch_save\n",
    "            batch_loss = 0\n",
    "            print(f\"batch {i}, Loss: {loss}\")\n",
    "\n",
    "            trajectories = odeint(model, X, torch.linspace(1, 0, 100).type(torch.float32), method=\"rk4\")\n",
    "            metrics = save(model, trajectories, metrics, X, Y, loss, i, args)      \n",
    "            \n",
    "    print(\"finished training\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\"\"\"combined training loop for rnode and cfm (if you want 'node+cfm' just set regularization constants to 0)\"\"\"\n",
    "def combined_training(args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    model = MLP()\n",
    "\n",
    "    initial_samples = sampler(args.initial_dataset)\n",
    "    target_samples = sampler(args.target_dataset)\n",
    "\n",
    "    X = initial_samples(1000)\n",
    "    Y = target_samples(1000)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    batch_save = int(args.num_batches/5)\n",
    "    batch_loss = 0\n",
    "\n",
    "    metrics = {\n",
    "        \"batches\": [],\n",
    "        \"loss\": [],\n",
    "        \"log_probability\": [],\n",
    "        \"flow_length\": [],\n",
    "        \"wasserstein2_distance\": []\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(1, args.num_batches+1), desc=\"Training Batches\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x0, x1 = initial_samples(args.batch_size), target_samples(args.batch_size)\n",
    "\n",
    "        loss_cfm = cfm_loss(model, x1, x0, args.batch_size)\n",
    "        loss_rnode = rnode_loss(model, x1, args)\n",
    "        loss = args.cfm_loss_coefficient * loss_cfm + args.rnode_loss_coefficient * loss_rnode\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss\n",
    "\n",
    "        if i == 1:\n",
    "            fig, ax = initial_setup(X, Y)\n",
    "            fig.savefig(os.path.join(args.parameter_path + \"/trajectories\", \"initial_target_distr.png\"))\n",
    "\n",
    "        elif i%batch_save == 0:\n",
    "            loss = batch_loss/batch_save\n",
    "            batch_loss = 0\n",
    "            print(f\"batch {i}, Loss: {loss}\")\n",
    "            \n",
    "            trajectories = odeint(model, X, torch.linspace(1, 0, 100).type(torch.float32), method=\"rk4\")\n",
    "            metrics = save(model, trajectories, metrics, X, Y, loss, i, args)         \n",
    "            \n",
    "    print(\"finished training\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main(): \n",
    "    \"\"\"\n",
    "    This can be called by running \"python -m training [PARAMETERS]\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model-type\", \n",
    "                        default=\"rnode\",\n",
    "                        choices=[\"rnode\", \"cfm\", \"combined\"])\n",
    "    \n",
    "    parser.add_argument(\"--initial-dataset\", \n",
    "        default=\"normal\",\n",
    "        choices=[\"normal\", \"moons\", \"gaussians\", \"circles\"])\n",
    "\n",
    "    parser.add_argument(\"--target-dataset\", \n",
    "        default=\"moons\", \n",
    "        choices=[\"normal\", \"moons\", \"gaussians\", \"circles\"])\n",
    "    \n",
    "    parser.add_argument(\"--num-batches\", default = 10000, type=int)  \n",
    "    parser.add_argument(\"--batch-size\", default = 200, type=int)\n",
    "    parser.add_argument(\"--learning-rate\", default = 0.0005 , type=float)\n",
    "    parser.add_argument(\"--seed\", default = 44, type=int)\n",
    "    parser.add_argument(\"--parameter-path\", default=\"likelihood:/\", type=str)\n",
    "\n",
    "    parser.add_argument(\"--odeint-method\", default = \"rk4\", type=str,\n",
    "                        choices=[\"dopri5\", \"dopri8\", \"adaptative_heun\", \"euler\", \"rk4\"])\n",
    "    parser.add_argument(\"--odeint-rtol\", default=1e-5, type=float)\n",
    "    parser.add_argument(\"--odeint-atol\", default=1e-5, type=float)\n",
    "\n",
    "    parser.add_argument(\"--reg-kinetic-energy\", default = 0, type=float)\n",
    "    parser.add_argument(\"--reg-frobenius-norm\", default = 0, type=float)\n",
    "    parser.add_argument(\"--sigma\", default=0, type=float)\n",
    "    parser.add_argument(\"--rnode-loss-coefficient\", default=1, type=float)\n",
    "    parser.add_argument(\"--cfm-loss-coefficient\", default=1, type=float)\n",
    "    args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model_type: str): #choices: \"cfm\" or \"rnode\"\n",
    "    args = argparse.Namespace(\n",
    "        model_type=model_type,\n",
    "        initial_dataset=\"normal\",\n",
    "        target_dataset=\"moons\",\n",
    "        num_batches=1000,\n",
    "        batch_size=256,\n",
    "        learning_rate=1e-3,\n",
    "        parameter_path=\"C:/\",\n",
    "        odeint_method=\"rk4\",\n",
    "        odeint_rtol=1e-3,\n",
    "        odeint_atol=1e-3,\n",
    "        reg_kinetic_energy=.1,\n",
    "        reg_frobenius_norm=.1,\n",
    "        seed=44,\n",
    "        sigma = 0,\n",
    "        rnode_loss_coefficient=1,\n",
    "        cfm_loss_coefficient=1\n",
    "    )\n",
    "\n",
    "    print(\"Args parsed!\", flush=True)\n",
    "    print(f\"Args: {args}\", flush=True)\n",
    "\n",
    "    hyperparameters = {\n",
    "    \"model_type\": args.model_type,\n",
    "    \"learning_rate\": args.learning_rate,\n",
    "    \"batch_size\": args.batch_size,\n",
    "    \"num_batches\": args.num_batches,\n",
    "    \"odeint_method\": args.odeint_method,\n",
    "    \"seed\": args.seed}\n",
    "\n",
    "\n",
    "    if args.model_type == \"rnode\":\n",
    "        args.parameter_path = args.parameter_path + f\"{args.model_type}_to_\" + f\"{args.target_dataset}_kin\" + f\"{args.reg_kinetic_energy}_frob\" + f\"{args.reg_frobenius_norm}\" \n",
    "        Path(os.path.join(args.parameter_path, \"trajectories\")).mkdir(parents=True, exist_ok=True)\n",
    "        Path(os.path.join(args.parameter_path, \"models\")).mkdir(parents=True, exist_ok=True)\n",
    "        hyperparameters[\"kinetic_regularization\"] = args.reg_kinetic_energy\n",
    "        hyperparameters[\"frobenius_regularization\"] = args.reg_frobenius_norm\n",
    "        metrics = rnode_training(args)\n",
    "\n",
    "    elif args.model_type == \"cfm\":\n",
    "        args.parameter_path = args.parameter_path + f\"{args.model_type}_{args.initial_dataset}_to_\" + f\"{args.target_dataset}_sigma\"+ f\"{args.sigma}\"\n",
    "        Path(os.path.join(args.parameter_path, \"trajectories\")).mkdir(parents=True, exist_ok=True)\n",
    "        Path(os.path.join(args.parameter_path, \"models\")).mkdir(parents=True, exist_ok=True)\n",
    "        hyperparameters[\"sigma\"] = args.sigma\n",
    "        metrics = cfm_training(args)\n",
    "\n",
    "    else:\n",
    "        args.parameter_path = args.parameter_path + f\"{args.model_type}_{args.initial_dataset}_to_\" + f\"{args.target_dataset}_rnode\"+ f\"{args.rnode_loss_coefficient}_cfm\" + f\"{args.cfm_loss_coefficient}\"\n",
    "        Path(os.path.join(args.parameter_path, \"trajectories\")).mkdir(parents=True, exist_ok=True)\n",
    "        Path(os.path.join(args.parameter_path, \"models\")).mkdir(parents=True, exist_ok=True)\n",
    "        hyperparameters[\"kinetic_regularization\"] = args.reg_kinetic_energy\n",
    "        hyperparameters[\"frobenius_regularization\"] = args.reg_frobenius_norm\n",
    "        hyperparameters[\"sigma\"] = args.sigma\n",
    "        hyperparameters[\"rnode_loss_coefficient\"] = args.rnode_loss_coefficient\n",
    "        hyperparameters[\"cfm_loss_coefficient\"] = args.cfm_loss_coefficient\n",
    "        metrics = combined_training(args)\n",
    "\n",
    "\n",
    "    save_hyperparameters_and_metrics(os.path.join(args.parameter_path, \"training_log.txt\"), hyperparameters, metrics)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
